{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aerial-massachusetts",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask.distributed\n",
    "import dask_jobqueue\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import dask_xgboost\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'SMC01/2021-05-11-ppdataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-centre",
   "metadata": {},
   "source": [
    "## 0.0 Start Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.SLURMCluster(config_name='slurm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-heritage",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.scale(jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-suggestion",
   "metadata": {},
   "source": [
    "## 1.0 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-suite",
   "metadata": {},
   "source": [
    "## 2.0 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error_2t'] = df['gdps_2t'] - df['obs_2t']\n",
    "df['step_hour'] = df['step'] / 3600\n",
    "df['step_td'] = dd.to_timedelta(df['step'], unit='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = []\n",
    "cont_columns = ['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', \n",
    "                'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', \n",
    "                'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick']\n",
    "target = 'error_2t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'min: {df.date.min().compute()}')\n",
    "print(f'max: {df.date.max().compute()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[df.date <= '2019-12-02'][[target, 'date', 'station'] + cat_columns + cont_columns]\n",
    "y_train = df[df.date <= '2019-12-02'][target]\n",
    "X_valid = df[(df.date > '2019-12-02') & (df.date <= '2020-01-01')][[target, 'date', 'station'] + cat_columns + cont_columns]\n",
    "y_valid = df[(df.date > '2019-12-02') & (df.date <= '2020-01-01')][target]\n",
    "X_test = df[df.date > '2020-01-01'][[target, 'date', 'station'] + cat_columns + cont_columns]\n",
    "y_test = df[df.date > '2020-01-01'][target]\n",
    "\n",
    "print(f'train: {len(X_train)/len(df)}, valid: {len(X_valid)/len(df)}, test: {len(X_test)/len(df)}')\n",
    "\n",
    "train_counts = X_train.date.value_counts().compute()\n",
    "valid_counts = X_valid.date.value_counts().compute()\n",
    "test_counts = X_test.date.value_counts().compute()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train_counts.index, train_counts, s=1, c='g')\n",
    "ax.scatter(valid_counts.index, valid_counts, s=1, c='b')\n",
    "ax.scatter(test_counts.index, test_counts, s=1, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-checkout",
   "metadata": {},
   "source": [
    "* Deal with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not X_train.isnull().values.any(), \"There are NaN values in the dataframe\"\n",
    "assert not X_valid.isnull().values.any(), \"There are NaN values in the dataframe\"\n",
    "assert not X_test.isnull().values.any(), \"There are NaN values in the dataframe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-supply",
   "metadata": {},
   "source": [
    "* We do not find all stations that we trained on in the valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stations = X_train.station.unique()\n",
    "valid_stations = X_valid.station.unique()\n",
    "test_stations = X_test.station.unique()\n",
    "print(f\"{len(set(train_stations) - set(valid_stations))} stations not in valid set\")\n",
    "print(f\"{len(set(train_stations) - set(test_stations))} stations not in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-profit",
   "metadata": {},
   "source": [
    "## 3.0 Training\n",
    "\n",
    "### 3.0.0 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(y_train).describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-alexandria",
   "metadata": {},
   "source": [
    "* No error (error_2t=0) can be a valid baseline, without abs() the mean is basically zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines['baseline_zero'] = 0\n",
    "y_train.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-practitioner",
   "metadata": {},
   "source": [
    "* La moyenne/médiane sur toutes les stations (train set) est un première baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines['baseline_mean_pos'] = abs(y_train).mean().compute()\n",
    "baselines['baseline_mean_neg'] = -abs(y_train).mean().compute()\n",
    "baselines['baseline_median_pos'] = abs(y_train).describe().compute().loc['50%']\n",
    "baselines['baseline_median_neg'] = -abs(y_train).describe().compute().loc['50%']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-shuttle",
   "metadata": {},
   "source": [
    "* La moyenne/médiane par station (train set) est un deuxième baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines['baseline_station_mean_pos'] = abs(X_train.groupby('station').error_2t.mean().compute())\n",
    "baselines['baseline_station_mean_neg'] = -abs(X_train.groupby('station').error_2t.mean().compute())\n",
    "# Not working with parallelization\n",
    "# baselines['baseline_station_median_pos'] = abs(df.iloc[X_train.index].groupby('station').error_2t.median())\n",
    "# baselines['baseline_station_median_neg'] = -abs(df.iloc[X_train.index].groupby('station').error_2t.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-sender",
   "metadata": {},
   "source": [
    "* Prendre l'erreur de l'année précédente pour un tuple (station, date, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['date_min'] = X_train.date.dt.strftime('%m-%d') \n",
    "baseline_last_year = X_train.groupby(['station', 'date_min']).error_2t.mean().compute().reset_index()\n",
    "baselines['baseline_last_year'] = baseline_last_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-crime",
   "metadata": {},
   "source": [
    "* Compute metrics for each baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in baselines.items():\n",
    "    print(k)\n",
    "    if 'station' in k:\n",
    "        predictions_ = dd.merge(X_test, v, on='station', suffixes=('', '_pred'), how='left').error_2t_pred.compute()\n",
    "        print(f'{predictions_.isna().sum()} NaN values: fill with 0.')\n",
    "        predictions_ = predictions_.fillna(0)\n",
    "    elif 'year' in k:\n",
    "        X_test['date_min'] = X_test.date.dt.strftime('%m-%d') \n",
    "        predictions_ = dd.merge(X_test, v, on=['station', 'date_min'], suffixes=('', '_pred'), how='left').error_2t_pred.compute()\n",
    "        print(f'{predictions_.isna().sum()} NaN values: fill with 0.')\n",
    "        predictions_ = predictions_.fillna(0)\n",
    "    else:\n",
    "        predictions_ = np.full(len(X_test), v)\n",
    "    print(f'\\tMAE: {mean_absolute_error(y_test, predictions_)}')\n",
    "    print(f'\\tRMSE: {mean_squared_error(y_test, predictions_, squared=False)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

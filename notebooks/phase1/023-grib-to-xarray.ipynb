{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lovely-catholic",
   "metadata": {},
   "source": [
    "# GRIB to XArray\n",
    "\n",
    "The purpose of this notebook is to read a bunch of GRIB files with PyGrib, and directly build an XArray dataset.\n",
    "The motivation is to avoid having an intermediary file format like NetCDF, and consequently save a lot on IO.\n",
    "\n",
    "It would be nice to do it using the cfgrib engine for XArray, but cfgrib makes it impractical to open all the fields we want, because it doesn't allow us to open multiple fields with different vertical levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-major",
   "metadata": {},
   "source": [
    "The overall strategy is to \n",
    "\n",
    "1. Read and filter an GRIB file\n",
    "2. Put the fields we want in an XArray Dataset\n",
    "3. Read many grib files in parallel.\n",
    "4. Merge the XArray datasets from different files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-reliance",
   "metadata": {},
   "source": [
    "## 1 Read and filter a grib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import pygrib\n",
    "from tqdm.notebook import tqdm\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(os.getenv('DATA_DIR'))\n",
    "GRIB_INPUT_DIR = DATA_DIR / 'data/gdps/2020020112'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_short_name(message):\n",
    "    to_extract = [\n",
    "        'al',\n",
    "        'hpbl',\n",
    "        'prate',\n",
    "        'prmsl',\n",
    "        'thick',\n",
    "        '10si',\n",
    "        '10wdir',\n",
    "        '10u',\n",
    "        '10v',\n",
    "        '2d',\n",
    "        '2r',\n",
    "        '2t',\n",
    "    ]\n",
    "    if message.shortName in to_extract:\n",
    "        return message.shortName\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortNameLevelExtractor:\n",
    "    def __init__(self, name, levels):\n",
    "        self.name = name\n",
    "        self.levels = levels\n",
    "        \n",
    "    def __call__(self, message):\n",
    "        if message.shortName == self.name:\n",
    "            for level in self.levels:\n",
    "                if message.level == level:\n",
    "                    return f'{self.name}_{level}'\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeExtractor:\n",
    "    def __init__(self, extractors):\n",
    "        self.extractors = extractors\n",
    "        \n",
    "    def __call__(self, message):\n",
    "        for e in self.extractors:\n",
    "            is_included = e(message)\n",
    "            if is_included:\n",
    "                return is_included\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = CompositeExtractor([\n",
    "    from_short_name,\n",
    "    ShortNameLevelExtractor('t', [850, 500]),\n",
    "    ShortNameLevelExtractor('gh', [1000, 850, 500]),\n",
    "    ShortNameLevelExtractor('q', [850, 500]),\n",
    "    ShortNameLevelExtractor('u', [500]),\n",
    "    ShortNameLevelExtractor('v', [500]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields(grib_iter, extractor):\n",
    "    fields = {}\n",
    "    for message in grib_iter:\n",
    "        label = extractor(message)\n",
    "        if label:\n",
    "            fields[label] = message\n",
    "            \n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_one_file(grib_file_path, extractor):\n",
    "    print(grib_file_path)\n",
    "    gribfile = pygrib.open(str(grib_file_path))\n",
    "    dataset = file_to_xarray(gribfile, extractor)\n",
    "    gribfile.close()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_to_xarray(pass_dir, extractor):\n",
    "    input_path = pathlib.Path(GRIB_INPUT_DIR)\n",
    "    input_files = sorted(list(input_path.glob('*.grib2')))\n",
    "    \n",
    "    input_files = input_files[:8]\n",
    "    \n",
    "    with multiprocessing.Pool() as pool:\n",
    "        datasets = pool.starmap(do_one_file, [(f, extractor) for f in input_files[:10]])\n",
    "    \n",
    "    return xr.concat(datasets, dim='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_xarray(grib_file, extractor):\n",
    "    sample = next(iter(grib_file))\n",
    "    \n",
    "    lats, lons = sample.latlons()\n",
    "    lats, lons = lats[:,0], lons[0]\n",
    "    \n",
    "    datetime = grib_date_to_pandas(sample.dataDate, sample.dataTime)\n",
    "    datetime = [datetime]\n",
    "    \n",
    "    if sample.stepUnits == 1:\n",
    "        step = [pd.Timedelta(sample.step, 'h')]\n",
    "    else:\n",
    "        raise ValueError('Unhandled step units')\n",
    "        \n",
    "    fields = {}\n",
    "    for msg in grib_file:\n",
    "        label = extractor(msg)\n",
    "        if label:\n",
    "            data_array = message_to_xarray(msg, lats, lons, step, datetime)\n",
    "            fields[label] = data_array\n",
    "            \n",
    "    return xr.Dataset(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_to_xarray(grib_message, lats, lons, step, datetime):\n",
    "    values = np.array(grib_message.values, dtype=np.float32)\n",
    "    values = np.expand_dims(values, axis=0)\n",
    "    values = np.expand_dims(values, axis=0)\n",
    "\n",
    "    da = xr.DataArray(\n",
    "        values, dims=['datetime', 'step', 'lat', 'lon'], \n",
    "        coords={'lat': lats, 'lon': lons, 'datetime': datetime, 'step': step}\n",
    "    )\n",
    "    \n",
    "    da.attrs['units'] = grib_message.units\n",
    "    \n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grib_date_to_pandas(date, time):\n",
    "    date, time = str(date), str(time)\n",
    "    date_string = f'{date[:4]}-{date[4:6]}-{date[6:8]} {time[:2]}:{time[2:4]}'\n",
    "    return pd.Timestamp(date_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset = pass_to_xarray(GRIB_INPUT_DIR, extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset = big_dataset.assign_coords(valid=lambda x: x.datetime + x.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = pathlib.Path(GRIB_INPUT_DIR)\n",
    "input_files = sorted(list(input_path.glob('*.grib2')))\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for f in input_files[8:12]:\n",
    "    gribfile = pygrib.open(str(f))\n",
    "    dataset = file_to_xarray(gribfile, extractor)\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = xr.merge(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ffc103-b17c-4c7d-80cc-ce4fd8e991d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.isel(step=0)['2t'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab37b2-6835-4ffa-9c75-7d3d486a2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.isel(step=1)['thick'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeof_dataset(dataset):\n",
    "    total = 0\n",
    "    for var in dataset.variables:\n",
    "        array = dataset[var]\n",
    "        total += array.size * array.dtype.itemsize\n",
    "        \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_dataset(d) / 1024**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "gribfile = pygrib.open(str(input_files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.Dataset(data_vars=data_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-outline",
   "metadata": {},
   "source": [
    "## 3. Read many grib files in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv('SLURM_CPUS_PER_TASK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_one_file(path):\n",
    "    gribfile = pygrib.open(str(path))\n",
    "    return file_to_xarray(gribfile, extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_list = input_files[:20]\n",
    "\n",
    "with multiprocessing.Pool(int(4)) as pool:\n",
    "    datasets = list(tqdm(pool.imap(do_one_file, in_list), total=len(in_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-correction",
   "metadata": {},
   "source": [
    "## 4. Interpolate at stations\n",
    "\n",
    "see notebooks 1901 1902\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "from smc01.interpolate.obs import MongoIEMDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URL = 'localhost'\n",
    "MONGO_PORT = 27017\n",
    "USERNAME = None\n",
    "PASSWORD = None\n",
    "ADMIN_DB = 'admin'\n",
    "COLLECTION = 'iem'\n",
    "DB = 'smc01_raw_obs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\n",
    "    host=MONGO_URL,\n",
    "    port=MONGO_PORT,\n",
    "    tz_aware=True,\n",
    "    authSource=ADMIN_DB,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = MongoIEMDatabase(client, DB, COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = big_dataset.valid.min().data.item()\n",
    "begin_date = datetime.datetime.utcfromtimestamp(begin_date // 1e9)\n",
    "\n",
    "end_date = big_dataset.valid.max().data.item()\n",
    "end_date = datetime.datetime.utcfromtimestamp(end_date // 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_info = db.station_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations = big_dataset.interp(\n",
    "    {\n",
    "        \"lat\": xr.DataArray(station_info[\"lat\"], dims=\"station\"),\n",
    "        \"lon\": xr.DataArray(station_info[\"lon\"], dims=\"station\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations.station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations = at_stations.assign_coords(\n",
    "    station=xr.DataArray(station_info[\"station\"], dims=\"station\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations.station[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_valid = {\n",
    "    valid_time: group for valid_time, group in at_stations.groupby(\"valid\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_valid_key = next(iter(by_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_valid = by_valid[one_valid_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step = one_valid.isel(stacked_datetime_step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "date, step = one_step.stacked_datetime_step.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "step.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.to_datetime(date, unit=\"s\")\n",
    "step = datetime.timedelta(hours=step.total_seconds() / 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "date.to_pydatetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset.data_vars.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in big_dataset.data_vars.keys():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-perry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMC01",
   "language": "python",
   "name": "smc01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

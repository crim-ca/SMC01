{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charming-berlin",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask.distributed\n",
    "import dask_jobqueue\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import dask_xgboost\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_USER\n",
    "\n",
    "sys.path.insert(0, '../smc01')\n",
    "from utils.splitter import BacktestSplitter\n",
    "from utils.mlflow import *\n",
    "from utils.logger import *\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-hawaii",
   "metadata": {},
   "source": [
    "## 1.0 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ''\n",
    "model = 'xgboost'\n",
    "bench = 'backtest'\n",
    "label = 'test'\n",
    "exp_suffix = 'test'\n",
    "rootdir = ''\n",
    "verbose = True\n",
    "user = ''\n",
    "mlflow_tracking_uri = 'mongodb://localhost:27017/mlflow'\n",
    "logger = get_logger(__name__)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start cluster\n",
    "cluster = dask_jobqueue.SLURMCluster(config_name='slurm', cores=10, processes=10)\n",
    "cluster.scale(jobs=3)\n",
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = dd.read_parquet(PATH)\n",
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df['error_2t'] = df['gdps_2t'] - df['obs_2t']\n",
    "df['step_hour'] = df['step'] / 3600\n",
    "df['step_td'] = dd.to_timedelta(df['step'], unit='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features selection\n",
    "cat_columns = []\n",
    "cont_columns = ['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', \n",
    "                'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', \n",
    "                'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick']\n",
    "target = 'error_2t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_experiment(bench, exp_suffix, model, label,\n",
    "                     rootdir='',\n",
    "                     verbose=True,\n",
    "                     user='',\n",
    "                     mlflow_tracking_uri=''):\n",
    "    '''\n",
    "    Start mlflow server first\n",
    "    '''\n",
    "\n",
    "    # Generate run name and experiment name\n",
    "    job_id = gen_run_id(model_name=model, label=label) # Generate run name, eg: 2021-05-18_163000-YMW-xgboost-test\n",
    "    exp_name = gen_exp_name(bench=bench, suffix=exp_suffix) # Generate experiment name, eg: backtest_test\n",
    "\n",
    "    # Create log dir\n",
    "    rootdir = Path(rootdir) / job_id\n",
    "    rootdir.mkdir(parents=True, exist_ok=False)\n",
    "    \n",
    "    # Configure logger (optional)\n",
    "    log_path = rootdir / \"run.log\"\n",
    "    log_level = logging.DEBUG if verbose else logging.INFO\n",
    "    configure_logger(path=log_path, level=log_level)\n",
    "    logger.info(f\"'{bench}' run started\")\n",
    "\n",
    "    # Dict of mlflow config\n",
    "    metadata = {\n",
    "        'exp_name': exp_name,\n",
    "        'job_id': job_id,\n",
    "        'log_path': log_path,\n",
    "        'job_rootdir': str(rootdir),\n",
    "        'mlflow_user': user,\n",
    "        'metrics': dict(),\n",
    "        'mlflow_tracking_uri': mlflow_tracking_uri,\n",
    "        'user': user\n",
    "    }\n",
    "\n",
    "    # Set tracking URI to communicate with mlflow server\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    metadata['mlflow_server'] = mlflow.tracking.get_tracking_uri()\n",
    "    logger.info(f\"MLFlow tracking enabled: {mlflow.tracking.get_tracking_uri()}\")\n",
    "\n",
    "    # Set experiment name and run name, start run\n",
    "    logger.info(f\"MLFlow Experiment name = {exp_name}\")\n",
    "    logger.info(f\"MLFlow Run name = {job_id}\")\n",
    "    mlflow.set_experiment(exp_name)\n",
    "    mlflow.start_run(run_name=job_id)\n",
    "    mlflow.set_tag(MLFLOW_USER, user)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(df, metadata, valid_duration='30D', test_frequency='2MS', test_n_periods=11, test_gap='0h',\n",
    "             cat_columns=[], cont_columns=['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', 'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', 'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick'], target='error_2t',\n",
    "             num_boost_round=100, params={'objective': 'reg:squarederror', 'tree_method': 'hist', 'eval_metric': ['rmse', 'mae'], 'eta': 0.3},\n",
    "             verbose=True):\n",
    "    \n",
    "    # Log mlflow parameters\n",
    "    mlflow_log_params(metadata)\n",
    "    \n",
    "    # Isolate selected features\n",
    "    X = df[['date', 'station'] + cat_columns + cont_columns]\n",
    "    y = df[['date', target]]\n",
    "    mlflow_log_params({'categoricals': cat_columns, 'continuous': cont_columns})\n",
    "    \n",
    "    # Instantiate backtest splitter\n",
    "    bs = BacktestSplitter(date_time_column='date', test_frequency=test_frequency, test_n_periods=test_n_periods, test_gap=test_gap)\n",
    "    mlflow_log_params(bs.get_params(), prefix='splitter')\n",
    "    n_splits = bs.get_n_splits(df)\n",
    "    mlflow_log_params({'n_splits': n_splits})\n",
    "    splits = list(enumerate(bs.get_split_indices(df)))\n",
    "    split_definitions = dict.fromkeys([str(i) for i in range(n_splits)])\n",
    "    logger.info(f'Processing {n_splits} backtest splits.')\n",
    "    \n",
    "    all_metrics = dict.fromkeys(range(n_splits))\n",
    "    for fold_i, (train_start, train_end, test_start, next_test_end) in splits:\n",
    "        logger.info(f'Backtest {fold_i}...')\n",
    "        if verbose:\n",
    "            artifact_path = f\"fold_{fold_i}\"\n",
    "            out_dir = '/'.join((metadata['job_rootdir'], artifact_path))\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # Define valid start and end date\n",
    "        valid_start = train_end - pd.to_timedelta(valid_duration)\n",
    "        valid_end = train_end\n",
    "        train_end = valid_start\n",
    "        \n",
    "        # Get splits\n",
    "        X_train = X[(X.date >= train_start) & (X.date < train_end)]\n",
    "        y_train = y[(y.date >= train_start) & (y.date < train_end)].drop('date', axis=1)\n",
    "        \n",
    "        X_valid = X[(X.date >= valid_start) & (X.date < valid_end)]\n",
    "        y_valid = y[(y.date >= valid_start) & (y.date < valid_end)].drop('date', axis=1)\n",
    "        \n",
    "        X_test = X[(X.date >= test_start) & (X.date < next_test_end)]\n",
    "        y_test = y[(y.date >= test_start) & (y.date < next_test_end)].drop('date', axis=1)\n",
    "        \n",
    "        if verbose:\n",
    "            logger.info(f\"TRAIN=[{train_start}, {train_end}[, VALID=[{valid_start}, {valid_end}[, TEST= [{test_start} - {next_test_end}[\")\n",
    "            # logger.info(f'train: {\"{:.0%}\".format(len(X_train)/len(X))}, valid: {\"{:.0%}\".format(len(X_valid)/len(X))}, test: {\"{:.0%}\".format(len(X_test)/len(X))}')\n",
    "        \n",
    "        # Save backtest split dates\n",
    "        bt_dict = dict()\n",
    "        bt_dict['train_start'] = train_start\n",
    "        bt_dict['train_end_date'] = train_end\n",
    "        bt_dict['valid_start'] = valid_start\n",
    "        bt_dict['valid_end_date'] = valid_end\n",
    "        bt_dict['test_start'] = test_start\n",
    "        bt_dict['test_end_date'] = next_test_end\n",
    "        split_definitions[str(fold_i)] = bt_dict\n",
    "                \n",
    "        if verbose:\n",
    "            # Split validation\n",
    "            train_stations = X_train.station.unique()\n",
    "            valid_stations = X_valid.station.unique()\n",
    "            test_stations = X_test.station.unique()\n",
    "            logger.info(f\"{len(set(train_stations) - set(valid_stations))} stations not in valid set\")\n",
    "            logger.info(f\"{len(set(train_stations) - set(test_stations))} stations not in test set\")\n",
    "\n",
    "            train_counts = X_train.date.value_counts().compute()\n",
    "            valid_counts = X_valid.date.value_counts().compute()\n",
    "            test_counts = X_test.date.value_counts().compute()\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.scatter(train_counts.index, train_counts, s=1, c='g')\n",
    "            ax.scatter(valid_counts.index, valid_counts, s=1, c='b')\n",
    "            ax.scatter(test_counts.index, test_counts, s=1, c='r')\n",
    "            ax.tick_params(axis='x', labelrotation = 45)\n",
    "            backtest_splits_path = str('/'.join((out_dir, 'backtest_splits.png')))\n",
    "            fig.savefig(backtest_splits_path, format='png')\n",
    "            mlflow.log_artifact(local_path=backtest_splits_path, artifact_path=artifact_path)\n",
    "                \n",
    "        # Convert to DMatrix for xgboost\n",
    "        X_train = X_train.drop(['date', 'station'], axis=1)\n",
    "        X_test = X_test.drop(['date', 'station'], axis=1)\n",
    "        X_valid = X_valid.drop(['date', 'station'], axis=1)\n",
    "        dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "        dvalid = xgb.dask.DaskDMatrix(client, X_valid, y_valid)\n",
    "        dtest = xgb.dask.DaskDMatrix(client, X_test)\n",
    "        \n",
    "        # Train XGBoost\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "        mlflow_log_params(params, prefix='model')\n",
    "        model = xgb.dask.train(client, params, dtrain, num_boost_round, evals=watchlist, verbose_eval=10, early_stopping_rounds=10)\n",
    "        \n",
    "        if verbose:\n",
    "            # Train validations\n",
    "            fig = plt.figure(figsize=(10,6))\n",
    "            plt.plot(model['history']['train']['rmse'], label='Train')\n",
    "            plt.plot(model['history']['valid']['rmse'], label='Valid')\n",
    "            plt.legend()\n",
    "            plt.xlabel('Iterations')\n",
    "            plt.ylabel('RMSE')\n",
    "            plt.title('RMSE Loss')\n",
    "            losses_path = str('/'.join((out_dir, 'losses.png')))\n",
    "            fig.savefig(losses_path, format='png')\n",
    "            mlflow.log_artifact(local_path=losses_path, artifact_path=artifact_path)\n",
    "        \n",
    "            # Feature importance\n",
    "            ax = xgb.plot_importance(model['booster'])\n",
    "            feature_importance_path = str('/'.join((out_dir, 'feature_importance.png')))\n",
    "            ax.figure.savefig(feature_importance_path, format='png')\n",
    "            mlflow.log_artifact(local_path=feature_importance_path, artifact_path=artifact_path)\n",
    "        \n",
    "        # Performance\n",
    "        predictions = xgb.dask.predict(client, model, dtest)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "        logger.info(f'MAE: {mae}')\n",
    "        logger.info(f'RMSE: {rmse}')\n",
    "        metrics = {'MAE': mae, 'RMSE': rmse}\n",
    "        mlflow.log_metrics({f\"fold_{k}\": v for k, v in metrics.items()}, step=fold_i)\n",
    "        all_metrics[fold_i] = metrics\n",
    "        \n",
    "        # del X_train, y_train, X_valid, y_valid, X_test, y_test, dtrain, dvalid, dtest, model, predictions\n",
    "        \n",
    "    # Log\n",
    "    mlflow_log_params({f'{fold}_{date}': value for fold, dates in split_definitions.items() for date, value in dates.items()}, prefix='split')\n",
    "    runs_avg_metrics = {f'avg_{metric}': v for metric, v in pd.DataFrame(all_metrics).mean(axis=1).to_dict().items()}\n",
    "    mlflow.log_metrics(runs_avg_metrics)\n",
    "    mlflow.log_artifact(metadata['log_path'])\n",
    "    \n",
    "    # End run\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    # del X, y\n",
    "    \n",
    "    return bs, X, y, split_definitions, model, predictions, all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-understanding",
   "metadata": {},
   "source": [
    "## 2.0 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = start_experiment(bench='temporal', exp_suffix='analysis', model='xgboost', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns=['step', 'gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', 'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', 'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick']\n",
    "bs, X, y, split_definitions, model, predictions, all_metrics = backtest(df, metadata, test_frequency='1YS', test_n_periods=1, num_boost_round=1000, verbose=False, cont_columns=cont_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-transcription",
   "metadata": {},
   "source": [
    "## 3.0 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_duration='30D'\n",
    "test_frequency='1YS'\n",
    "test_n_periods=1\n",
    "test_gap='0h'\n",
    "cat_columns=[]\n",
    "cont_columns=['step', 'gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', 'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', 'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick']\n",
    "target='error_2t'\n",
    "num_boost_round=1000\n",
    "params={'objective': 'reg:squarederror', 'tree_method': 'hist', 'eval_metric': ['rmse', 'mae'], 'eta': 0.3}\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = list(enumerate(bs.get_split_indices(df)))\n",
    "fold_i, (train_start, train_end, test_start, next_test_end) = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define valid start and end date\n",
    "valid_start = train_end - pd.to_timedelta(valid_duration)\n",
    "valid_end = train_end\n",
    "train_end = valid_start\n",
    "\n",
    "# Get splits\n",
    "df_train = df[(df.date >= train_start) & (df.date < train_end)]\n",
    "df_valid = df[(df.date >= valid_start) & (df.date < valid_end)]\n",
    "df_test = df[(df.date >= test_start) & (df.date < next_test_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df_test.compute()\n",
    "results['error_2t_predictions'] = predictions.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-medicine",
   "metadata": {},
   "source": [
    "* Similar distribution between real errors and predicted errors\n",
    "* Less variance for predictions\n",
    "* Predictions slightly more on the negative side, like the real errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(results['error_2t'], bins=np.arange(-20, 20, 1), alpha=0.5, label='Ground Truth')\n",
    "ax.hist(results['error_2t_predictions'], bins=np.arange(-20, 20, 1), alpha=0.5, label='Predictions')\n",
    "fig.suptitle('Distributions of real errors and predicted errors')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-amplifier",
   "metadata": {},
   "source": [
    "* Most of the errors are just a couple of celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(abs(results['error_2t'] - results['error_2t_predictions']), bins=np.arange(0, 20, 1), alpha=0.5)\n",
    "fig.suptitle('Distribution of absolute difference between ground truth and predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-recognition",
   "metadata": {},
   "source": [
    "* The prediction error seems lower generally when the observed temperature is higher.\n",
    "* **Try with other features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = results['obs_2t'].quantile([0, 0.25, 0.5, 0.75, 1])\n",
    "fig, ax = plt.subplots(4, 1, figsize=(10,15), sharex=True, sharey=True)\n",
    "for i, (bot, top) in enumerate([(0, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1)]):\n",
    "    sub_df = results[(results.obs_2t >= quantiles[bot]) & (results.obs_2t < quantiles[top])]\n",
    "    errors = abs(sub_df['error_2t'] - sub_df['error_2t_predictions'])\n",
    "    ax[i].hist(errors, bins=np.arange(0, 20, 1), alpha=0.5);\n",
    "    ax[i].text(1.1, 0.5, f'Mean: {np.around(errors.mean(), 2)}', transform=ax[i].transAxes)\n",
    "    ax[i].set_title(f'{round(quantiles[bot])} - {round(quantiles[top])} (degC)')\n",
    "plt.tight_layout();\n",
    "fig.suptitle('Error distribution based on observed temperature', y=1.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-revelation",
   "metadata": {},
   "source": [
    "* Does not seem to have any significant trend (over or undershooting predictions) between east and west or south/north\n",
    "* Mean absolute error seems higher for northern stations and higher for eastern stations\n",
    "* Predictions seems worse where the density of weather stations is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['error'] = results['error_2t'] - results['error_2t_predictions']\n",
    "results['abs_error'] = abs(results['error_2t'] - results['error_2t_predictions'])\n",
    "lon_lat_impact = results.groupby(['longitude', 'latitude'])['error'].mean().reset_index().sort_values(['longitude', 'latitude'])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18,6))\n",
    "ax[0].grid(axis='y')\n",
    "ax[0].axhline(0, color='black', lw=0.75)\n",
    "ax[0].scatter(x=lon_lat_impact['longitude'], y=lon_lat_impact['error'], alpha=0.1)\n",
    "ax[0].set_title('Longitude')\n",
    "ax[1].grid(axis='y')\n",
    "ax[1].axhline(0, color='black', lw=0.75)\n",
    "ax[1].scatter(x=lon_lat_impact['latitude'], y=lon_lat_impact['error'], alpha=0.1)\n",
    "ax[1].set_title('Latitude');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_lat_impact = results.groupby(['longitude', 'latitude'])['abs_error'].mean().reset_index().sort_values(['longitude', 'latitude'])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18,6))\n",
    "z = np.polyfit(lon_lat_impact['longitude'], lon_lat_impact['abs_error'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax[0].grid(axis='y')\n",
    "ax[0].plot(lon_lat_impact['longitude'], p(lon_lat_impact['longitude']), \"b--\")\n",
    "ax[0].scatter(x=lon_lat_impact['longitude'], y=lon_lat_impact['abs_error'], alpha=0.1)\n",
    "ax[0].set_title('Longitude')\n",
    "z = np.polyfit(lon_lat_impact['latitude'], lon_lat_impact['abs_error'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax[1].grid(axis='y')\n",
    "ax[1].plot(lon_lat_impact['latitude'], p(lon_lat_impact['latitude']), \"b--\")\n",
    "ax[1].scatter(x=lon_lat_impact['latitude'], y=lon_lat_impact['abs_error'], alpha=0.1)\n",
    "ax[1].set_title('Latitude');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_lat_impact.plot.scatter(x='longitude', y='latitude', c='abs_error', figsize=(16,8), vmax=5)\n",
    "plt.xlim(-200,0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-baltimore",
   "metadata": {},
   "source": [
    "* Mean absolute error per station is between 1 and 4.\n",
    "* Distribution of absolute difference between ground truth and predictions (**grouped by stations**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_mean = results.groupby('station')['abs_error'].mean()\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(station_mean, bins=np.arange(0, 10, 0.5), alpha=0.5)\n",
    "fig.suptitle('Distribution of absolute difference between ground truth and predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-wealth",
   "metadata": {},
   "source": [
    "* Having a single model per station does not seem to give better results than having a single model for all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_tracking_uri = 'mongodb://localhost:27017/mlflow'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "station_experiments = mlflow.search_runs(experiment_ids='2')\n",
    "# station_experiments.to_csv('../temporal_stations_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MAE: {station_experiments[\"metrics.avg_MAE\"].mean()}')\n",
    "print(f'RMSE: {station_experiments[\"metrics.avg_RMSE\"].mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-bermuda",
   "metadata": {},
   "source": [
    "* The bigger the step, the higher the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,6))\n",
    "results.groupby('step')['error'].mean().plot(ax=ax[0])\n",
    "ax[0].set_title('Mean error per step')\n",
    "results.groupby('step')['abs_error'].mean().plot(ax=ax[1])\n",
    "ax[1].set_title('Mean absolute error per step');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-stanley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

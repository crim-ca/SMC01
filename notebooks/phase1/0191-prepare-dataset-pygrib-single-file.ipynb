{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset using pygrib\n",
    "\n",
    "The purpose of this notebook is to prepare a statistical downscaling dataset\n",
    "using pygrib and XArray. Notebook *018-prepare-dataset.ipynb* showed that \n",
    "it is not really practical to do so with XArray and CFGrib only. Indeed,\n",
    "opening a large collection of grib files is very long with CFGrib, and \n",
    "the API limitations on filtering the fields to have only consistent\n",
    "datacubes force us to open the files multiple times.\n",
    "\n",
    "Our overall strategy here is going to be:\n",
    "1. Extract relevant data from grib to NetCDF using PyGrib.\n",
    "2. The NetCDF will not be CF compliant, it will only be an\n",
    "   intermediate file format.\n",
    "3. Open the NetCDF files using XArray, and then perform the\n",
    "   interpolation.\n",
    "  \n",
    "Our hope is that XArray will be much happier playing with netCDF files \n",
    "than Grib files, and that we will be able to open and interpolate the \n",
    "data only once using XArray and dropping strict CF convention compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract relevant data using pygrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.distributed\n",
    "import dask_jobqueue\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import netCDF4\n",
    "import os\n",
    "import pathlib\n",
    "import pygrib\n",
    "import xarray as xr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(os.getenv('DATA_DIR'))\n",
    "GDPS_DIR = DATA_DIR / '2021-02-08-one-month-sample/'\n",
    "NETCDF_DIR = DATA_DIR / 'hdd_scratch/smc01/2021-02-09-one-month-single-file'\n",
    "\n",
    "TO_EXTRACT = [\n",
    "    'prmsl',\n",
    "    'hpbl',\n",
    "    'prate',\n",
    "    '2t',\n",
    "    '2d',\n",
    "    '2r',\n",
    "    '10si',\n",
    "    '10wdir',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.SLURMCluster(\n",
    "    env_extra=[\n",
    "        'source ~/.bash_profile','conda activate smc01'],\n",
    "    name='smc01-dask',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps_path = pathlib.Path(GDPS_DIR)\n",
    "output_path = pathlib.Path(NETCDF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grib_files = sorted(list(gdps_path.glob('*.grib2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grib_files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out how many dates and steps there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_strings = [g.stem[22:32] for g in grib_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_passes = sorted(list(set(pass_strings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pass = len(unique_passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_strings = [g.stem[-4:] for g in grib_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_strings[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_steps = sorted(list(set(step_strings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = len(unique_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grib_files[0].stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file_name = grib_files[0].stem + '_filtered.nc'\n",
    "target_file_path = output_path / target_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [\n",
    "    lambda x: x.shortName == 'st' and x.typeOfLevel == 'surface',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_lambda(message):\n",
    "    for l in lambdas:\n",
    "        if l(message):\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_to_datetime(pass_string):\n",
    "    print(pass_string)\n",
    "    return datetime.datetime(\n",
    "        int(pass_string[:4]),\n",
    "        int(pass_string[4:6]),\n",
    "        int(pass_string[6:8]),\n",
    "        int(pass_string[8:10])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_netcdf_file(output_file, date, step):\n",
    "    time_units = 'seconds since 1970-01-01 00:00:00.0'\n",
    "\n",
    "    output_file.createDimension('latitude', size=751)\n",
    "    output_file.createDimension('longitude', size=1500)\n",
    "    output_file.createDimension('time', size=1)\n",
    "    output_file.createDimension('step', size=1)\n",
    "\n",
    "    step_var = output_file.createVariable('step', 'i4', dimensions=('step'))\n",
    "    step_var[0] = step\n",
    "\n",
    "    time_var = output_file.createVariable('time', 'f8', dimensions=('time'))\n",
    "    float_date = netCDF4.date2num(date, time_units)\n",
    "    time_var[0] = float_date\n",
    "    time_var.units = time_units\n",
    "\n",
    "    variables = {}\n",
    "    for short_name in TO_EXTRACT:\n",
    "        variables[short_name] = output_file.createVariable(\n",
    "            short_name, 'f4', dimensions=('time', 'step', 'latitude', 'longitude'),\n",
    "            zlib=True) \n",
    "        \n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_latlon_to_file(output_file, lat, lon):\n",
    "    lat_var = output_file.createVariable('latitude', 'f4', dimensions=('latitude'))\n",
    "    lat_var[:] = lat\n",
    "\n",
    "    lon_var = output_file.createVariable('longitude', 'f4', dimensions=('longitude'))\n",
    "    lon_var[:] = lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_one_file(input_dir, pass_string, step_string, output_dir):\n",
    "    print(pass_string)\n",
    "    target_file_path = output_dir / (f'gdps_{pass_string}_{step_string}.nc')\n",
    "    \n",
    "    if target_file_path.is_file():\n",
    "        print(f'Skipping {pass_string} because output file already exists')\n",
    "    \n",
    "    output_file = netCDF4.Dataset(str(target_file_path), 'w')\n",
    "    date = pass_to_datetime(pass_string)\n",
    "    \n",
    "    step = int(step_string[1:])\n",
    "    \n",
    "    variables = prepare_netcdf_file(output_file, date, step)\n",
    "\n",
    "    print(pass_string, step_string)\n",
    "    grib_file_name = f'CMC_glb_latlon.24x.24_{pass_string}_{step_string}.grib2'\n",
    "    grib_file_path = input_dir / grib_file_name\n",
    "\n",
    "    grib_file = pygrib.open(str(grib_file_path))\n",
    "\n",
    "    lat, lon = grib_file[1].latlons()\n",
    "    lat = lat[:,0]\n",
    "    lon = lon[0]\n",
    "\n",
    "    add_latlon_to_file(output_file, lat, lon)\n",
    "\n",
    "    for message in grib_file:\n",
    "        if message.shortName in TO_EXTRACT:\n",
    "            var = variables[message.shortName]\n",
    "            var[0,0,:] = message.values\n",
    "\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in unique_passes[:1]:\n",
    "    for s in unique_steps[:1]:\n",
    "        handle_one_file(gdps_path, p, s, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(processes=8) as pool:\n",
    "    pool.starmap(handle_one_pass, [(gdps_path, p, output_path) for p in unique_passes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_one_file_delayed = dask.delayed(handle_one_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayeds = [handle_one_file_delayed(gdps_path, p, s, output_path) for p in unique_passes for s in unique_steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayeds[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(delayeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(*delayeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file_name = grib_files[0].stem + '_filtered.nc'\n",
    "target_file_path = output_path / target_file_name\n",
    "\n",
    "grib_file.seek(0)\n",
    "for grib_file_path in grib_files:\n",
    "    grib_file = pygrib.open(str(grib_file_path))\n",
    "    for message in grib_file:\n",
    "        if message.shortName in to_extract or compound_lambda(message):\n",
    "            print(message)\n",
    "            \n",
    "    grib_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = grib_file[1].latlons()\n",
    "lat = lat[:,0]\n",
    "lon = lon[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = netCDF4.Dataset(str(target_file_path), 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.createDimension('latitude', size=lat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.createDimension('longitude', size=lon.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.createDimension('time', size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.createDimension('step', size=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_var = root.createVariable('latitude', 'f4', dimensions=('latitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_var[:] = lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_var = root.createVariable('longitude', 'f4', dimensions=('longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_var[:] = lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grib_file.seek(0)\n",
    "for message in grib_file:\n",
    "    if message.shortName in to_extract or compound_lambda(message):\n",
    "        var = root.createVariable(message.shortName, 'f4', dimensions=('latitude', 'longitude'))\n",
    "        var[:] = message.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open using Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nest_filenames(files):\n",
    "    passes = {}\n",
    "    for f in files:\n",
    "        pass_name = f.stem[5:15]\n",
    "        \n",
    "        pass_list = passes.get(pass_name, [])\n",
    "        pass_list.append(f)\n",
    "        passes[pass_name] = pass_list\n",
    "        \n",
    "    sorted_passes = sorted(passes.keys())\n",
    "        \n",
    "    return [passes[k] for k in sorted_passes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_filenames = sorted(list(output_path.glob('*.nc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_filenames[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_nc = nest_filenames(nc_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps = xr.open_mfdataset(\n",
    "    nested_nc, concat_dim=['time', 'step'], \n",
    "    combine='nested', parallel=True, compat='no_conflicts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps.sel(time=datetime.datetime(2020,7,22,12), step=15)['2t'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps.mean(dim='time').sel(step=0)['2t'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

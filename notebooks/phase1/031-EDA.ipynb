{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275f1504-7399-4438-a6cb-a4746974f3de",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4d766-8a06-4fee-91eb-3a6ebc1e42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns \n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import graphviz\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12013346-91e2-4eee-a5ae-9291d9704d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5dd75-0987-4425-9cc1-e5416e66d066",
   "metadata": {},
   "source": [
    "## 1.0 Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5fd94-8420-4bc9-96cb-52ede5779c87",
   "metadata": {},
   "source": [
    "* Selon la convention, il semble qu'il y ait des forecasts fait 2x par jour? À minuit et à midi? `2020123100 -> 31 decembre 2020 a minuit et 2020123112 -> 31 decembre 2020 a midi`?\n",
    "    * Oui c'est ça.\n",
    "    * En UTC\n",
    "* Garder seulement les données des derniers mois de 2020 pour rentrer en mémoire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be84d679-c8c1-4487-9337-45c2b298d3bc",
   "metadata": {},
   "source": [
    "Add\n",
    "\n",
    "```python\n",
    "df['error_2t'] = df['gdps_2t'] - df['obs_2t']\n",
    "df['squared_error_2t'] = (df['gdps_2t'] - df['obs_2t']) ** 2\n",
    "df['step_hour'] = df['step'] / 3600\n",
    "df['step_td'] = dd.to_timedelta(df['step'], unit='S')\n",
    "df['valid'] = df['date'] + df['step_td']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40bcdc-7008-455b-94f8-02aa766147e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(PATH + '/2019010100.parquet')\n",
    "# df = pd.read_parquet(PATH)\n",
    "df = pd.concat([pd.read_parquet(path) for path in glob.glob(PATH + '/*', recursive=True) if path.split('/')[-1][:5] == '20201'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da07c7-fa8e-4def-853e-84999a97eb6b",
   "metadata": {},
   "source": [
    "* La date, on assume que c'est à minuit? Et on ajoute le step (sec) pour avoir la bonne heure à partir de là?\n",
    "    * UTC midi et minuit\n",
    "* C'est quoi toutes les features `gdps`?\n",
    "    * obs = vient des stations (pas le droit pour predire), c'est les mesures, on les a pas dans le fond au moment de la prediction/inference\n",
    "    * gdps = vient du modele physique meteo, on peut prendre\n",
    "* Les features `obs` c'est observé j'imagine? Ils veulent dire quoi sinon?\n",
    "    * voir ci-haut\n",
    "    \n",
    "```python\n",
    "['gdps_prate', qte de pluie/precipitation prevue\n",
    " 'gdps_prmsl', pression, correlated avec altitude\n",
    " 'gdps_2t', temperature a 2m\n",
    " 'gdps_2d', point de rosee\n",
    " 'gdps_2r', humidite relative\n",
    " 'gdps_10u', composante de vent a 10m\n",
    " 'gdps_10v', composante de vent a 10m\n",
    " 'gdps_10si', vitesse m/s\n",
    " 'gdps_10wdir', composante vent p/r au nord, ca peut flipper et fucker meme si cest presque pareil (359deg vs 1deg)\n",
    " 'gdps_al', albedo (reflexivite de la surface de la terre)\n",
    " 'gdps_t_850', temperature a la hauteur ou pression est 850 Pa\n",
    " 'gdps_t_500', temperature a la hauteur ou pression est 500 Pa\n",
    " 'gdps_gh_1000', altitude en m ou la pression est 1000 Pa\n",
    " 'gdps_gh_850', altitude en m ou la pression est 850 Pa\n",
    " 'gdps_gh_500', altitude en m ou la pression est 500 Pa\n",
    " 'gdps_u_500', composante de vent ou la pression est 500 Pa\n",
    " 'gdps_v_500', composante de vent ou la pression est 500 Pa\n",
    " 'gdps_q_850', pression atmospherique ou la pression est 850 Pa\n",
    " 'gdps_q_500', pression atmospherique ou la pression est 500 Pa\n",
    " 'gdps_thick', difference de hauteur entre 2 couches]\n",
    "```\n",
    " \n",
    "* On cherche relation entre les variables: quand temperature haute eleve, on sous-estime la temperature\n",
    "* Observation frequency change par station, verifier. Devrait etre 8 observations au moins par jour.\n",
    "* Match previson et observation, par station. Donc normal que ce soit 8 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9423a-82d3-41cd-8132-9f9603b6debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2f53f-0f68-4dfa-8f10-82f368d81002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f787b-d61f-4820-8ab5-ddd12f9880af",
   "metadata": {},
   "source": [
    "## 2.0 Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca24458-bee4-41dc-be24-7250a22e57af",
   "metadata": {},
   "source": [
    "* Comment on va splitter? Temporel par station (on prend pour chaque station toutes les données avant la date X pour train, le reste pour valid), temporel seulement (sans notion de station, on prend toutes les données avant la date X pour train, le reste pour valid), par station (X stations pour train, Y stations pour valid)\n",
    "* Le target de notre modèle c'est l'erreur entre la variable prédite et celle observée en réalité? `df['gdps_2t'] - df['obs_2t']`?\n",
    "    * 2t = 2m du sol, temperature de surface\n",
    "    * On pourrait target vitesse du vent aussi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb516b-cd6c-4792-bcb3-3e2f795b0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error_2t'] = df['gdps_2t'] - df['obs_2t']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "df[lambda x: x.station == 'CYBR']['error_2t'].plot(ax=axes[0])\n",
    "axes[0].set_title('CYBR station')\n",
    "df.groupby('step')['error_2t'].mean().plot(ax=axes[1])\n",
    "axes[1].set_title('Mean of all stations');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c4817-7a6d-4a11-bfa0-512f08c7c1d1",
   "metadata": {},
   "source": [
    "* On dirait la shape de l'amérique du Nord. Sounds good. L'installation de geopandas marche pas pour une raison quelconque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0046ac-5e88-4b8e-bf24-d4dbef425c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['longitude','latitude']).size().reset_index().rename(columns={0:'count'}).plot.scatter(x='longitude', y='latitude', c='green')\n",
    "plt.xlim(-200,0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a46e8-076a-4ee2-bf36-69e135ac1ec5",
   "metadata": {},
   "source": [
    "* La majorité des mesures proviennent du Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78c806-8e34-4177-a24a-f29fd7e5012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['longitude','latitude']).size().reset_index().rename(columns={0:'count'}).plot.scatter(x='longitude', y='latitude', c='count')\n",
    "plt.xlim(-200,0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a009a-a29b-4f7c-94e9-bd3a8c75d00a",
   "metadata": {},
   "source": [
    "* Mesures assez constantes dans le temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67c483-3ce3-4edc-be08-ec1878c6de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('date').size().to_frame().rename(columns={0:'count'}).plot.line();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f38f6a-c977-471c-a11a-a8f746749434",
   "metadata": {},
   "source": [
    "* Stations do not have the same timespan of data points\n",
    "    * Check `2021-04-23-coverages.csv` pour avoir coverage (proportion, par station, du nombre de jours avec au moins 8 observations sur tous les jours)\n",
    "    * Confirmer avec mes parquets que le coverage fonctionne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676e701-6e38-4a8c-a69c-83daa4f95618",
   "metadata": {},
   "outputs": [],
   "source": [
    "timespan = df.groupby('station').agg({'date': [min, max]})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,30))\n",
    "ticklabels = timespan.index\n",
    "ticks = np.arange(len(ticklabels))\n",
    "\n",
    "ax.hlines(ticks, timespan['date']['min'], timespan['date']['max'], linewidth=1)\n",
    "# ax.set_yticks(ticks)\n",
    "# ax.set_yticklabels(ticklabels)\n",
    "xfmt = mdates.DateFormatter('%Y/%m/%d\\n%H:%M:%S')\n",
    "ax.xaxis.set_major_formatter(xfmt)\n",
    "ax.set_xticks(pd.date_range(min(timespan['date']['min']), max(timespan['date']['max']), periods=5).tolist())\n",
    "# ax.grid(axis='y')\n",
    "ax.set_title('Time range for each station');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5078142-17a7-47f9-bec9-e3ee92a4d329",
   "metadata": {},
   "source": [
    "* The dataframe is already sorted. The next line has the same date or is a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a1cea-ee84-42e7-bcc4-e4a5e281655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "((df.date - df.shift().date)).value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47624b2-696b-47fe-b030-f663fe5cf7eb",
   "metadata": {},
   "source": [
    "## 3.0 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983470f3-bfb7-473c-a774-dba5bf695ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9754d63-630a-409c-a105-c7c47973a77c",
   "metadata": {},
   "source": [
    "* Définition des features catégorielles/continues.\n",
    "* Comment utiliser les variables obs?\n",
    "* Utiliser station comme feature? Elevation? Step? Date? Lon/Lat? Le choix de les ignorer provient du notebook 027-dask-analysis. On veut être le plus général possible?\n",
    "    * Modele generique ou par station (donc utiliser comme features)?\n",
    "    * Un modele par station?\n",
    "    * Fonction qui prend n'importe quel modele et essayer differents modeles sur chaque station.\n",
    "    * Modele lineaire comme baseline+ (reg. lin. serait bon first step, compromis avec modele complexe)\n",
    "    * Utiliser la date comme features c'est bon.\n",
    "    * Sin wave pour encoder temporal features (voir notebook david)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e94045-5fc1-4b86-9334-a7a8f38e72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = []\n",
    "cont_columns = ['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', \n",
    "                'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', \n",
    "                'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick']\n",
    "target = 'error_2t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b6a75-2a86-42f4-b6c7-487dc687f4e6",
   "metadata": {},
   "source": [
    "* Split temporel naïf: on prendre X premières semaines pour train, X suivantes pour valid, X suivantes pour test\n",
    "* 70/10/20\n",
    "* shuffle=False pour garder l'ordre temporel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee66f58-2eb8-4b55-86b3-5bad1d1ac4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[cat_columns + cont_columns]\n",
    "y = df[target]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.30, shuffle=False)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=2/3, shuffle=False)\n",
    "\n",
    "assert abs(len(X_train) - 0.7*len(X)) < 1\n",
    "assert abs(len(X_valid) - 0.1*len(X)) < 1\n",
    "assert abs(len(X_test) - 0.2*len(X)) < 1\n",
    "assert df.iloc[X_train.index[-1]].date <= df.iloc[X_valid.index[0]].date\n",
    "assert df.iloc[X_valid.index[-1]].date <= df.iloc[X_test.index[0]].date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d2eaf-80ab-4c4d-bb7d-8a18e4b8052d",
   "metadata": {},
   "source": [
    "* Deal with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa577e-c601-44a7-a359-a064ffa0f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not X.isnull().values.any(), \"There are NaN values in the dataframe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fe9b5-50e9-4082-a57a-a6fb03528113",
   "metadata": {},
   "source": [
    "* We do not find all stations that we trained on in the valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59d564-ce75-42ae-9030-014e6e535142",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stations = df.iloc[X_train.index].station.unique()\n",
    "valid_stations = df.iloc[X_valid.index].station.unique()\n",
    "test_stations = df.iloc[X_test.index].station.unique()\n",
    "print(f\"{len(set(train_stations) - set(valid_stations))} stations not in valid set\")\n",
    "print(f\"{len(set(train_stations) - set(test_stations))} stations not in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794519e1-7942-4361-8199-b38eb7064251",
   "metadata": {},
   "source": [
    "* DMatrix is an internal data structure that is used by XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd30b1-04e4-4101-9f39-5837a010ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086efce9-48e8-418d-9555-8e65a4e98141",
   "metadata": {},
   "source": [
    "## 4.0 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471a816-a7ef-40cb-a617-1d1a60f6f264",
   "metadata": {},
   "source": [
    "### 4.0.0 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af463566-cfe9-4436-8419-cd811d1a75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d50ed-2fb4-4b4a-b1e0-38153b52fc47",
   "metadata": {},
   "source": [
    "* En moyenne, la prédiction est off de 3.13 degC par rapport à la valeur observée. La médiane est de 2.14 degC d'erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716eea1e-726c-468d-920a-30be0c6d304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = df.iloc[X_train.index].error_2t\n",
    "abs(errors).describe().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e5e32-fe97-4f4e-a8ce-7fbcbbff916e",
   "metadata": {},
   "source": [
    "* No error (error_2t=0) can be a valid baseline, without abs() the mean is basically zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38d4a0-e369-457d-8511-4df0dee05a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines['baseline_zero'] = 0\n",
    "errors.describe().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee40d6b-e38d-4e34-8e37-c7ca5acea53f",
   "metadata": {},
   "source": [
    "* La moyenne/médiane sur toutes les stations (train set) est un première baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e38c5-144e-4996-b4ab-247cecdb5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines['baseline_mean_pos'] = abs(errors).mean()\n",
    "baselines['baseline_mean_neg'] = -abs(errors).mean()\n",
    "baselines['baseline_median_pos'] = abs(errors).median()\n",
    "baselines['baseline_median_neg'] = -abs(errors).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0bdfe-daa3-48d5-8a37-8ccfbfff7af7",
   "metadata": {},
   "source": [
    "* La moyenne/médiane par station (train set) est un deuxième baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8536707-5fb1-4895-8cd2-6c018106a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines['baseline_station_mean_pos'] = abs(df.iloc[X_train.index].groupby('station').error_2t.mean())\n",
    "baselines['baseline_station_mean_neg'] = -abs(df.iloc[X_train.index].groupby('station').error_2t.mean())\n",
    "baselines['baseline_station_median_pos'] = abs(df.iloc[X_train.index].groupby('station').error_2t.median())\n",
    "baselines['baseline_station_median_neg'] = -abs(df.iloc[X_train.index].groupby('station').error_2t.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9314f2-9dc7-4aad-90c4-cef3227865ab",
   "metadata": {},
   "source": [
    "* Prendre l'erreur de l'année précédente pour un tuple (station, date, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf4ddc-3208-4b6c-96a0-0070abd96081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not enough data analyzed for now because of memories issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21175428-15e1-4306-bb4d-368e515fddcb",
   "metadata": {},
   "source": [
    "* Compute metrics for each baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828dac8-1e27-4364-a8f4-874d010d2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in baselines.items():\n",
    "    if 'station' in k:\n",
    "        predictions_ = df.iloc[X_test.index].merge(v, on='station', suffixes=('', '_pred')).error_2t_pred\n",
    "    else:\n",
    "        predictions_ = np.full(len(X_test), v)\n",
    "    print(k)\n",
    "    print(f'\\tMAE: {mean_absolute_error(y_test, predictions_)}')\n",
    "    print(f'\\tRMSE: {mean_squared_error(y_test, predictions_, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7ceb3-fa97-4864-8d8d-7e170109e7fd",
   "metadata": {},
   "source": [
    "### 4.0.1 Train XGboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1531a-823e-4652-bfab-32f0a790d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 100\n",
    "params = {'objective': 'reg:squarederror', \n",
    "          'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "          'eval_metric': ['rmse', 'mae']\n",
    "         }\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "evals_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b6f58-9de9-4c7d-baa8-df70cbd17a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, verbose_eval=10, \n",
    "                  early_stopping_rounds=10, evals_result=evals_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3bf05b-9bb1-4f43-a6ed-aea2e61d6413",
   "metadata": {},
   "source": [
    "* Why is RMSE loss lower for valid set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c07388-cfeb-46da-8f42-2b9c0d225f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(evals_result['train']['rmse'], label='Train')\n",
    "plt.plot(evals_result['valid']['rmse'], label='Valid')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724b3cd-3b21-4cd0-adae-991099c51c92",
   "metadata": {},
   "source": [
    "* J'aimerais analyser l'importance mais je ne sais pas ce que signifie chaque feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6812c-d697-4d91-9593-61b5c19294d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb3713-a104-4018-ae57-3c4d4286ffb5",
   "metadata": {},
   "source": [
    "* J'aimerais analyser l'arbre mais je ne sais pas ce que signifie chaque feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00c477-2015-4040-93f5-7014adfe0443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double click for bigger view\n",
    "fig, ax = plt.subplots(figsize=(200,10))\n",
    "xgb.plot_tree(model, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713bd62-00c6-4bec-b662-284878ff34b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(dtest)\n",
    "print(f'MAE: {mean_absolute_error(y_test, predictions)}')\n",
    "print(f'RMSE: {mean_squared_error(y_test, predictions, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c6e0b-0d82-4e77-a597-8f7822cdf63f",
   "metadata": {},
   "source": [
    "### 4.0.2 Train XGboost model, split temporal by dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29b286-c4e5-48da-a1a4-4a830bfb75a9",
   "metadata": {},
   "source": [
    "* Let's keep 2020-10-01 to 2020-12-01 as train set (2 months)\n",
    "* 2020-12-02 to 2020-12-14 as valid set (2 weeks)\n",
    "* 2020-12-15 to 2020-12-31 as test set (2 weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f2d38-1127-45ec-9f67-b0bf0daf1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'min: {df.date.min()}')\n",
    "print(f'max: {df.date.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22efcb-345e-4276-8ad2-db1bd9fd27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = df[lambda x: x.date <= '2020-12-01'].index\n",
    "valid_idx = df[lambda x: (x.date > '2020-12-01') & (x.date <= '2020-12-14')].index\n",
    "test_idx = df[lambda x: x.date > '2020-12-14'].index\n",
    "\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "X_valid = X.iloc[valid_idx]\n",
    "y_valid = y.iloc[valid_idx]\n",
    "X_test = X.iloc[test_idx]\n",
    "y_test = y.iloc[test_idx]\n",
    "\n",
    "print(f'train: {len(X_train)/len(X)}, valid: {len(X_valid)/len(X)}, test: {len(X_test)/len(X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa6633-d0f2-41f8-b5e0-c95b636a7458",
   "metadata": {},
   "source": [
    "* We do not find all stations that we trained on in the valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16957d6-b773-4ee2-9255-5733073ed59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stations = df.iloc[X_train.index].station.unique()\n",
    "valid_stations = df.iloc[X_valid.index].station.unique()\n",
    "test_stations = df.iloc[X_test.index].station.unique()\n",
    "print(f\"{len(set(train_stations) - set(valid_stations))} stations not in valid set\")\n",
    "print(f\"{len(set(train_stations) - set(test_stations))} stations not in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34beeaf2-8f1e-439b-8011-4fab56c46459",
   "metadata": {},
   "source": [
    "* DMatrix is an internal data structure that is used by XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be62e50-e1d5-4e02-8a24-3a39efc58cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435466e4-cd02-4771-a862-f206af53d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 100\n",
    "params = {'objective': 'reg:squarederror', \n",
    "          'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "          'eval_metric': ['rmse', 'mae'],\n",
    "          'eta': 0.01         }\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "evals_result = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbbf52-885a-478c-9640-f522cb2f4f99",
   "metadata": {},
   "source": [
    "* With default like 4.0.1 we cannot converge.\n",
    "* With lower lr (0.01) slightly better.\n",
    "* Higher or lower max_depth does not do much.\n",
    "* Difficult to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01a94c-1d51-455d-b1e6-fb0a74bc6540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, verbose_eval=10, \n",
    "                  early_stopping_rounds=10, evals_result=evals_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830046a-3b1b-4e9c-b237-d5b923bd0732",
   "metadata": {},
   "source": [
    "* Now RMSE loss is not lower for valid set, which is normal, what is wrong in 4.0.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee95451-dc76-466f-905f-d0dbb8a4fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(evals_result['train']['rmse'], label='Train')\n",
    "plt.plot(evals_result['valid']['rmse'], label='Valid')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946fcb0d-d0d5-41f9-91d7-5b7c5911e6da",
   "metadata": {},
   "source": [
    "* J'aimerais analyser l'importance mais je ne sais pas ce que signifie chaque feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742659e5-d001-4fde-bda2-dd8428b40c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecedcbf-ba74-4188-9fcd-817ce9958ff5",
   "metadata": {},
   "source": [
    "* J'aimerais analyser l'arbre mais je ne sais pas ce que signifie chaque feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbdc7f-dce9-4b7c-95db-0258d06c6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double click for bigger view\n",
    "fig, ax = plt.subplots(figsize=(200,10))\n",
    "xgb.plot_tree(model, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029751e-c7d9-4d4f-9d17-87734c562421",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(dtest)\n",
    "print(f'MAE: {mean_absolute_error(y_test, predictions)}')\n",
    "print(f'RMSE: {mean_squared_error(y_test, predictions, squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d861dd-da03-4e9f-9510-5da1154cdb21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smc)",
   "language": "python",
   "name": "smc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275f1504-7399-4438-a6cb-a4746974f3de",
   "metadata": {},
   "source": [
    "# XGBoost Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724e158-748e-4e84-9994-e9b105debf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4d766-8a06-4fee-91eb-3a6ebc1e42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask.distributed\n",
    "import dask_jobqueue\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import dask_xgboost\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import mlflow\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_USER\n",
    "\n",
    "sys.path.insert(0, '../smc01')\n",
    "from utils.splitter import BacktestSplitter\n",
    "from utils.mlflow import *\n",
    "from utils.logger import *\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12013346-91e2-4eee-a5ae-9291d9704d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ''\n",
    "model = 'xgboost'\n",
    "bench = 'backtest'\n",
    "label = 'test'\n",
    "exp_suffix = 'test'\n",
    "rootdir = ''\n",
    "verbose = True\n",
    "user = ''\n",
    "mlflow_tracking_uri = 'mongodb://localhost:27017/mlflow'\n",
    "logger = get_logger(__name__)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7da470-4227-4acd-a546-aa32bc23be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start cluster\n",
    "cluster = dask_jobqueue.SLURMCluster(config_name='slurm')\n",
    "cluster.scale(jobs=4)\n",
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c4288-f874-4166-af32-beca8420c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = dd.read_parquet(PATH)\n",
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498ee78-f949-497d-84f3-25774218c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df['error_2t'] = df['gdps_2t'] - df['obs_2t']\n",
    "df['step_hour'] = df['step'] / 3600\n",
    "df['step_td'] = dd.to_timedelta(df['step'], unit='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e94045-5fc1-4b86-9334-a7a8f38e72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features selection\n",
    "cat_columns = []\n",
    "cont_columns = ['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', \n",
    "                'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', \n",
    "                'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick']\n",
    "target = 'error_2t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f8dca7-949b-44fc-b8c6-c9a0c2c3827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_experiment(bench, exp_suffix, model, label,\n",
    "                     rootdir='',\n",
    "                     verbose=True,\n",
    "                     user='',\n",
    "                     mlflow_tracking_uri='mongodb://localhost:27017/mlflow'):\n",
    "    '''\n",
    "    Start mlflow server first\n",
    "    '''\n",
    "\n",
    "    # Generate run name and experiment name\n",
    "    job_id = gen_run_id(model_name=model, label=label) # Generate run name, eg: 2021-05-18_163000-YMW-xgboost-test\n",
    "    exp_name = gen_exp_name(bench=bench, suffix=exp_suffix) # Generate experiment name, eg: backtest_test\n",
    "\n",
    "    # Create log dir\n",
    "    rootdir = Path(rootdir) / job_id\n",
    "    rootdir.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "    # Configure logger (optional)\n",
    "    log_path = rootdir / \"run.log\"\n",
    "    log_level = logging.DEBUG if verbose else logging.INFO\n",
    "    configure_logger(path=log_path, level=log_level)\n",
    "    logger.info(f\"'{bench}' run started\")\n",
    "\n",
    "    # Dict of mlflow config\n",
    "    metadata = {\n",
    "        'exp_name': exp_name,\n",
    "        'job_id': job_id,\n",
    "        'log_path': log_path,\n",
    "        'job_rootdir': str(rootdir),\n",
    "        'mlflow_user': user,\n",
    "        'metrics': dict(),\n",
    "        'mlflow_tracking_uri': mlflow_tracking_uri,\n",
    "        'user': user\n",
    "    }\n",
    "\n",
    "    # Set tracking URI to communicate with mlflow server\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    metadata['mlflow_server'] = mlflow.tracking.get_tracking_uri()\n",
    "    logger.info(f\"MLFlow tracking enabled: {mlflow.tracking.get_tracking_uri()}\")\n",
    "\n",
    "    # Set experiment name and run name, start run\n",
    "    logger.info(f\"MLFlow Experiment name = {exp_name}\")\n",
    "    logger.info(f\"MLFlow Run name = {job_id}\")\n",
    "    mlflow.set_experiment(exp_name)\n",
    "    mlflow.start_run(run_name=job_id)\n",
    "    mlflow.set_tag(MLFLOW_USER, user)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c666969-3cf0-4a1a-8664-35de612f4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(df, metadata, valid_duration='30D', test_frequency='2MS', test_n_periods=11, test_gap='0h',\n",
    "             cat_columns=[], cont_columns=['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', 'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', 'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick'], target='error_2t',\n",
    "             num_boost_round=100, params={'objective': 'reg:squarederror', 'tree_method': 'hist', 'eval_metric': ['rmse', 'mae'], 'eta': 0.3}):\n",
    "    \n",
    "    # Log mlflow parameters\n",
    "    mlflow_log_params(metadata)\n",
    "    \n",
    "    # Isolate selected features\n",
    "    X = df[['date', 'station'] + cat_columns + cont_columns]\n",
    "    y = df[['date', target]]\n",
    "    mlflow_log_params({'categoricals': cat_columns, 'continuous': cont_columns})\n",
    "    \n",
    "    # Instantiate backtest splitter\n",
    "    bs = BacktestSplitter(date_time_column='date', test_frequency=test_frequency, test_n_periods=test_n_periods, test_gap=test_gap)\n",
    "    mlflow_log_params(bs.get_params(), prefix='splitter')\n",
    "    n_splits = bs.get_n_splits(df)\n",
    "    mlflow_log_params({'n_splits': n_splits})\n",
    "    splits = list(enumerate(bs.get_split_indices(df)))\n",
    "    split_definitions = dict.fromkeys([str(i) for i in range(n_splits)])\n",
    "    logger.info(f'Processing {n_splits} backtest splits.')\n",
    "    \n",
    "    all_metrics = dict.fromkeys(range(n_splits))\n",
    "    for fold_i, (train_start, train_end, test_start, next_test_end) in splits:\n",
    "        logger.info(f'Backtest {fold_i}...')\n",
    "        artifact_path = f\"fold_{fold_i}\"\n",
    "        out_dir = '/'.join((metadata['job_rootdir'], artifact_path))\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # Define valid start and end date\n",
    "        valid_start = train_end - pd.to_timedelta(valid_duration)\n",
    "        valid_end = train_end\n",
    "        train_end = valid_start\n",
    "        \n",
    "        # Get splits\n",
    "        X_train = X[(X.date >= train_start) & (X.date < train_end)]\n",
    "        y_train = y[(y.date >= train_start) & (y.date < train_end)].drop('date', axis=1)\n",
    "        \n",
    "        X_valid = X[(X.date >= valid_start) & (X.date < valid_end)]\n",
    "        y_valid = y[(y.date >= valid_start) & (y.date < valid_end)].drop('date', axis=1)\n",
    "        \n",
    "        X_test = X[(X.date >= test_start) & (X.date < next_test_end)]\n",
    "        y_test = y[(y.date >= test_start) & (y.date < next_test_end)].drop('date', axis=1)\n",
    "        \n",
    "        logger.info(f\"TRAIN=[{train_start}, {train_end}[, VALID=[{valid_start}, {valid_end}[, TEST= [{test_start} - {next_test_end}[\")\n",
    "        # logger.info(f'train: {\"{:.0%}\".format(len(X_train)/len(X))}, valid: {\"{:.0%}\".format(len(X_valid)/len(X))}, test: {\"{:.0%}\".format(len(X_test)/len(X))}')\n",
    "        \n",
    "        # Save backtest split dates\n",
    "        bt_dict = dict()\n",
    "        bt_dict['train_start'] = train_start\n",
    "        bt_dict['train_end_date'] = train_end\n",
    "        bt_dict['valid_start'] = valid_start\n",
    "        bt_dict['valid_end_date'] = valid_end\n",
    "        bt_dict['test_start'] = test_start\n",
    "        bt_dict['test_end_date'] = next_test_end\n",
    "        split_definitions[str(fold_i)] = bt_dict\n",
    "                \n",
    "        # Split validation\n",
    "        train_stations = X_train.station.unique()\n",
    "        valid_stations = X_valid.station.unique()\n",
    "        test_stations = X_test.station.unique()\n",
    "        logger.info(f\"{len(set(train_stations) - set(valid_stations))} stations not in valid set\")\n",
    "        logger.info(f\"{len(set(train_stations) - set(test_stations))} stations not in test set\")\n",
    "        \n",
    "        train_counts = X_train.date.value_counts().compute()\n",
    "        valid_counts = X_valid.date.value_counts().compute()\n",
    "        test_counts = X_test.date.value_counts().compute()\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(train_counts.index, train_counts, s=1, c='g')\n",
    "        ax.scatter(valid_counts.index, valid_counts, s=1, c='b')\n",
    "        ax.scatter(test_counts.index, test_counts, s=1, c='r')\n",
    "        ax.tick_params(axis='x', labelrotation = 45)\n",
    "        backtest_splits_path = str('/'.join((out_dir, 'backtest_splits.png')))\n",
    "        fig.savefig(backtest_splits_path, format='png')\n",
    "        mlflow.log_artifact(local_path=backtest_splits_path, artifact_path=artifact_path)\n",
    "                \n",
    "        # Convert to DMatrix for xgboost\n",
    "        X_train = X_train.drop(['date', 'station'], axis=1)\n",
    "        X_test = X_test.drop(['date', 'station'], axis=1)\n",
    "        X_valid = X_valid.drop(['date', 'station'], axis=1)\n",
    "        dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "        dvalid = xgb.dask.DaskDMatrix(client, X_valid, y_valid)\n",
    "        dtest = xgb.dask.DaskDMatrix(client, X_test)\n",
    "        \n",
    "        # Train XGBoost\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "        mlflow_log_params(params, prefix='model')\n",
    "        model = xgb.dask.train(client, params, dtrain, num_boost_round, evals=watchlist, verbose_eval=10, early_stopping_rounds=10)\n",
    "        \n",
    "        # Train validation\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        plt.plot(model['history']['train']['rmse'], label='Train')\n",
    "        plt.plot(model['history']['valid']['rmse'], label='Valid')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('RMSE Loss')\n",
    "        losses_path = str('/'.join((out_dir, 'losses.png')))\n",
    "        fig.savefig(losses_path, format='png')\n",
    "        mlflow.log_artifact(local_path=losses_path, artifact_path=artifact_path)\n",
    "        \n",
    "        # Feature importance\n",
    "        ax = xgb.plot_importance(model['booster'])\n",
    "        feature_importance_path = str('/'.join((out_dir, 'feature_importance.png')))\n",
    "        ax.figure.savefig(feature_importance_path, format='png')\n",
    "        mlflow.log_artifact(local_path=feature_importance_path, artifact_path=artifact_path)\n",
    "        \n",
    "        # Performance\n",
    "        predictions = xgb.dask.predict(client, model, dtest)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "        logger.info(f'MAE: {mae}')\n",
    "        logger.info(f'RMSE: {rmse}')\n",
    "        metrics = {'MAE': mae, 'RMSE': rmse}\n",
    "        mlflow.log_metrics({f\"fold_{k}\": v for k, v in metrics.items()}, step=fold_i)\n",
    "        all_metrics[fold_i] = metrics\n",
    "        \n",
    "        del X_train, y_train, X_valid, y_valid, X_test, y_test, dtrain, dvalid, dtest\n",
    "        \n",
    "    # Log\n",
    "    mlflow_log_params({f'{fold}_{date}': value for fold, dates in split_definitions.items() for date, value in dates.items()}, prefix='split')\n",
    "    runs_avg_metrics = {f'avg_{metric}': v for metric, v in pd.DataFrame(all_metrics).mean(axis=1).to_dict().items()}\n",
    "    mlflow.log_metrics(runs_avg_metrics)\n",
    "    mlflow.log_artifact(metadata['log_path'])\n",
    "    \n",
    "    # End run\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    return split_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32149b54-225d-4543-963c-05276935c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = start_experiment(bench='bbb', exp_suffix='test', model='xgboost', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff256a7d-54b5-4d18-8230-176628f4a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de10a6-a5bb-4aaa-bd62-d2127baeff27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sd = backtest(df, metadata, test_frequency='1YS', test_n_periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5e28f-e7ed-409f-a97b-f11889a9f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = start_experiment(bench='backtest', exp_suffix='test', model='xgboost', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48c02a-0e8e-4ef8-9085-ad6cd7d62b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sd = backtest(df, metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMC01",
   "language": "python",
   "name": "smc01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

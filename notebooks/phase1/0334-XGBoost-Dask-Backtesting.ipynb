{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275f1504-7399-4438-a6cb-a4746974f3de",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724e158-748e-4e84-9994-e9b105debf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4d766-8a06-4fee-91eb-3a6ebc1e42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask.distributed\n",
    "import dask_jobqueue\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import dask_xgboost\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12013346-91e2-4eee-a5ae-9291d9704d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81670260-ca54-4ad9-a3c7-02339c1d311b",
   "metadata": {},
   "source": [
    "## 0.0 Start Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41362f-2ed2-4c33-acb5-34fc1796c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.SLURMCluster(config_name='slurm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6231c1-6fa1-496d-b969-9a03c30ad8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba075d-a204-4655-8788-f528650035e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767c38e-5824-420f-9459-7647616231e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5dd75-0987-4425-9cc1-e5416e66d066",
   "metadata": {},
   "source": [
    "## 1.0 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1dbce5-a503-41ed-b9b0-1a0580d74287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328d4cb-0a5a-4b8e-867e-0008e119ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b781cc8-3b1b-46cf-a2bd-4c38e48ef440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9423a-82d3-41cd-8132-9f9603b6debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2f53f-0f68-4dfa-8f10-82f368d81002",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47624b2-696b-47fe-b030-f663fe5cf7eb",
   "metadata": {},
   "source": [
    "## 2.0 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498ee78-f949-497d-84f3-25774218c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error_2t'] = df['gdps_2t'] - df['obs_2t']\n",
    "df['step_hour'] = df['step'] / 3600\n",
    "df['step_td'] = dd.to_timedelta(df['step'], unit='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4c44f-6b1d-4ffa-a20e-ec638895e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e94045-5fc1-4b86-9334-a7a8f38e72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = []\n",
    "cont_columns = ['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', \n",
    "                'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', \n",
    "                'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick']\n",
    "target = 'error_2t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e161c84-4699-460a-9326-88f92e741059",
   "metadata": {},
   "source": [
    "## 3.0 Training\n",
    "\n",
    "### 3.0.0 Train XGboost model, backtesting\n",
    "\n",
    "* Let's train with X months, validate with 3 months and test with 3 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c666969-3cf0-4a1a-8664-35de612f4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(df, train_start=3, period=3, valid_split=3, test_split=3,\n",
    "             cat_columns=[], cont_columns=['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', 'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', 'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick'], target='error_2t',\n",
    "             num_boost_round=100, params={'objective': 'reg:squarederror', 'tree_method': 'hist', 'eval_metric': ['rmse', 'mae'], 'eta': 0.3}):\n",
    "    \n",
    "    # Isolate selected features\n",
    "    X = df[['date', 'station'] + cat_columns + cont_columns]\n",
    "    y = df[['date', target]]\n",
    "    \n",
    "    # Get min and max date\n",
    "    start_date = df.date.min().compute()\n",
    "    end_date = df.date.max().compute()\n",
    "    total_months = round(((end_date - start_date)/np.timedelta64(1, 'M')))\n",
    "    \n",
    "    # Dict of splits\n",
    "    split_idx = np.arange(0, total_months-test_split-valid_split, period)\n",
    "    split_definitions = dict.fromkeys(range(len(split_idx)))\n",
    "    \n",
    "    for idx in split_idx:\n",
    "        backtest_id = int(idx/period)\n",
    "        print(f'Backtest {backtest_id}...')\n",
    "        \n",
    "        # Get splits\n",
    "        train_end_date = start_date + datetime.timedelta(days=train_start*30) + datetime.timedelta(days=int(idx*30))\n",
    "        X_train = X[(X.date >= start_date) & (X.date <= train_end_date)]\n",
    "        y_train = y[(y.date >= start_date) & (y.date <= train_end_date)].drop('date', axis=1)\n",
    "        \n",
    "        valid_end_date = train_end_date + datetime.timedelta(days=valid_split*30)\n",
    "        X_valid = X[(X.date > train_end_date) & (X.date <= valid_end_date)]\n",
    "        y_valid = y[(y.date > train_end_date) & (y.date <= valid_end_date)].drop('date', axis=1)\n",
    "        \n",
    "        test_end_date = valid_end_date + datetime.timedelta(days=test_split*30)\n",
    "        X_test = X[(X.date > valid_end_date) & (X.date <= test_end_date)]\n",
    "        y_test = y[(y.date > valid_end_date) & (y.date <= test_end_date)].drop('date', axis=1)\n",
    "        \n",
    "        print(f'train: {len(X_train)/len(X)}, valid: {len(X_valid)/len(X)}, test: {len(X_test)/len(X)}')\n",
    "        \n",
    "        # Save backtest split dates\n",
    "        bt_dict = dict()\n",
    "        bt_dict['train_start'] = start_date\n",
    "        bt_dict['train_end_date'] = train_end_date\n",
    "        bt_dict['valid_start'] = train_end_date\n",
    "        bt_dict['valid_end_date'] = valid_end_date\n",
    "        bt_dict['test_start'] = valid_end_date\n",
    "        bt_dict['test_end_date'] = test_end_date\n",
    "        split_definitions[backtest_id] = bt_dict\n",
    "                \n",
    "        # Split validation\n",
    "        train_stations = X_train.station.unique()\n",
    "        valid_stations = X_valid.station.unique()\n",
    "        test_stations = X_test.station.unique()\n",
    "        print(f\"{len(set(train_stations) - set(valid_stations))} stations not in valid set\")\n",
    "        print(f\"{len(set(train_stations) - set(test_stations))} stations not in test set\")\n",
    "        \n",
    "        train_counts = X_train.date.value_counts().compute()\n",
    "        valid_counts = X_valid.date.value_counts().compute()\n",
    "        test_counts = X_test.date.value_counts().compute()\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(train_counts.index, train_counts, s=1, c='g')\n",
    "        ax.scatter(valid_counts.index, valid_counts, s=1, c='b')\n",
    "        ax.scatter(test_counts.index, test_counts, s=1, c='r')\n",
    "        plt.show();\n",
    "        \n",
    "        print(f'Train: {X_train.date.min().compute()} -> {X_train.date.max().compute()}, Valid: {X_valid.date.min().compute()} -> {X_valid.date.max().compute()}, Test: {X_test.date.min().compute()} -> {X_test.date.max().compute()}')        \n",
    "        \n",
    "        # Convert to DMatrix for xgboost\n",
    "        X_train = X_train.drop(['date', 'station'], axis=1)\n",
    "        X_test = X_test.drop(['date', 'station'], axis=1)\n",
    "        X_valid = X_valid.drop(['date', 'station'], axis=1)\n",
    "        dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "        dvalid = xgb.dask.DaskDMatrix(client, X_valid, y_valid)\n",
    "        dtest = xgb.dask.DaskDMatrix(client, X_test)\n",
    "        \n",
    "        # Train XGBoost\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "        model = xgb.dask.train(client, params, dtrain, num_boost_round, evals=watchlist, verbose_eval=10, early_stopping_rounds=10)\n",
    "        \n",
    "        # Train validation\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(model['history']['train']['rmse'], label='Train')\n",
    "        plt.plot(model['history']['valid']['rmse'], label='Valid')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('RMSE Loss')\n",
    "        plt.show();\n",
    "        \n",
    "        # Feature importance\n",
    "        xgb.plot_importance(model['booster'])\n",
    "        plt.show();\n",
    "        \n",
    "        # Performance\n",
    "        predictions = xgb.dask.predict(client, model, dtest)\n",
    "        print(f'MAE: {mean_absolute_error(y_test, predictions)}')\n",
    "        print(f'RMSE: {mean_squared_error(y_test, predictions, squared=False)}')\n",
    "    \n",
    "    return split_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de10a6-a5bb-4aaa-bd62-d2127baeff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = backtest(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b47431-7948-491a-929b-4980c6d017df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smc01-xgb)",
   "language": "python",
   "name": "smc01-xgb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

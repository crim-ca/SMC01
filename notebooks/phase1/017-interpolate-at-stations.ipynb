{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDPS Error by first interpolating the model at station coordinates\n",
    "\n",
    "The purpose of this file is to try out a new strategy for error computation of a model output.\n",
    "The idea is as follows:\n",
    "\n",
    "1. Load the model array\n",
    "2. Fetch all station coordinates\n",
    "3. Interpolate the model at station coordinates\n",
    "4. Flatten the interpolated values to a Pandas Dataframe (or a Dask Dataframe if necessary).\n",
    "\n",
    "The resulting dataframe should have columns such as\n",
    "- model\n",
    "- step\n",
    "- time\n",
    "- t2m_gdps\n",
    "- station\n",
    "- lat\n",
    "- lon\n",
    "- t2m_obs\n",
    "Then it should be easy to compute the error and make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import dask_jobqueue\n",
    "import dask.distributed\n",
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import seaborn as sns\n",
    "import time\n",
    "import xarray as xr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(os.getenv('DATA_DIR'))\n",
    "GDPS_DIR = DATA_DIR / 'data/2021-02-02-one-week-sample/'\n",
    "\n",
    "MONGO_URL = 'localhost'\n",
    "MONGO_PORT = 27017\n",
    "USERNAME = None\n",
    "PASSWORD = None\n",
    "ADMIN_DB = 'admin'\n",
    "DB = 'smc01_raw_obs_test'\n",
    "COLLECTION = 'iem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.SLURMCluster(\n",
    "    env_extra=[\n",
    "        'source ~/.bash_profile','conda activate smc01'],\n",
    "    name='smc01-dask',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load model array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps_files = sorted(list(pathlib.Path(GDPS_DIR).glob('CMC_glb_latlon.24x.24_*.grib2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps_files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nest_filenames(files):\n",
    "    passes = {}\n",
    "    for f in files:\n",
    "        pass_name = f.stem[22:32]\n",
    "        \n",
    "        pass_list = passes.get(pass_name, [])\n",
    "        pass_list.append(f)\n",
    "        passes[pass_name] = pass_list\n",
    "        \n",
    "    sorted_passes = sorted(passes.keys())\n",
    "        \n",
    "    return [passes[k] for k in sorted_passes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_files = nest_filenames(gdps_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps = xr.open_mfdataset(\n",
    "    nested_files, engine='cfgrib', concat_dim=['time', 'step'], \n",
    "    combine='nested', parallel=True, compat='no_conflicts',\n",
    "    backend_kwargs={'filter_by_keys': {\n",
    "        'typeOfLevel': 'heightAboveGround',\n",
    "        'stepType': 'instant',\n",
    "}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdps = gdps.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fetch station coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = gdps.valid_time.min().data.item()\n",
    "begin_date = datetime.datetime.utcfromtimestamp(begin_date // 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = gdps.valid_time.max().data.item()\n",
    "end_date = datetime.datetime.utcfromtimestamp(end_date // 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient(host=MONGO_URL, port=MONGO_PORT, username=USERNAME, password=PASSWORD, authSource=ADMIN_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mongo_client.smc01_raw_obs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db.iem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'valid': {\n",
    "        '$gte': begin_date + datetime.timedelta(days=1),\n",
    "        '$lt': end_date\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = collection.distinct('station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_infos = []\n",
    "\n",
    "for station in stations:\n",
    "    one_obs = collection.find_one({'station': station})\n",
    "    station_infos.append({\n",
    "        'station': station,\n",
    "        'lat': one_obs['lat'],\n",
    "        'lon': one_obs['lon']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = pd.DataFrame(station_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interpolate model at stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdps.t2m.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations = gdps.interp({\n",
    "    'latitude': xr.DataArray(station_df['lat'], dims='station'),\n",
    "    'longitude': xr.DataArray(station_df['lon'], dims='station'),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations.t2m.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations = at_stations.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fetch observations and find appropriate model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        '$addFields': {\n",
    "            'minute': {\n",
    "                '$minute': '$valid'\n",
    "            },\n",
    "            'hour': {\n",
    "                '$hour': '$valid'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$match': {\n",
    "            'minute': 0,\n",
    "            'valid': {\n",
    "                '$gte': begin_date,\n",
    "                '$lt': end_date\n",
    "            },\n",
    "            'hour': {\n",
    "                '$in': [0, 3, 6, 9, 12, 15, 18, 21]\n",
    "            },\n",
    "            'tmpf': {\n",
    "                '$exists': True,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': \"$station\",\n",
    "            'obs': {\n",
    "                '$push': '$$ROOT'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_obs_by_station = list(collection.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.find_one({'valid': begin_date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_by_station = {d['_id']: d['obs'] for d in mongo_obs_by_station}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_by_station['CYUL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in observations_by_station:\n",
    "    for obs in observations_by_station[station]:\n",
    "        if 'tmpf' not in obs:\n",
    "            print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_stations.t2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_station_reports(observations, at_station):\n",
    "    reports = []\n",
    "    \n",
    "    groups = {\n",
    "        valid_time: group \n",
    "        for valid_time, group in list(at_station.groupby('valid_time'))\n",
    "    }\n",
    "\n",
    "    for obs in observations:\n",
    "        obs_time = np.datetime64(obs['valid'], 'ns')\n",
    "        obs_temp = (obs['tmpf'] - 32) * (5/9) \n",
    "\n",
    "        if obs_time in groups:\n",
    "            group_of_time = groups[obs_time]\n",
    "\n",
    "            for i in range(len(group_of_time.stacked_step_time)):\n",
    "                date = datetime.datetime.utcfromtimestamp(group_of_time.time[i].item() / 1e9)\n",
    "                step = datetime.timedelta(seconds=group_of_time.step[i].item() / 1e9)\n",
    "                temp = group_of_time.t2m[i].item() - 273.15\n",
    "\n",
    "                reports.append({\n",
    "                    'station': obs['station'],\n",
    "                    'date': date,\n",
    "                    'step': step,\n",
    "                    'gdps_temp': temp,\n",
    "                    'obs_temp': obs_temp,\n",
    "                    'lat': obs['lat'],\n",
    "                    'lon': obs['lon'],\n",
    "                    'elevation': obs['elevation']\n",
    "                })\n",
    "            \n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_reports = []\n",
    "compute_delayed = dask.delayed(compute_station_reports)\n",
    "#compute_delayed = compute_station_reports\n",
    "\n",
    "for i, station in enumerate(stations[0:30]):\n",
    "   \n",
    "    if station in observations_by_station:\n",
    "        begin_sel = time.time()\n",
    "        at_station = at_stations.sel({\n",
    "            'station': i\n",
    "        })\n",
    "        \n",
    "        groups = {\n",
    "            valid_time: group \n",
    "            for valid_time, group in list(at_station.groupby('valid_time'))\n",
    "        }\n",
    "        \n",
    "        station_observations = observations_by_station[station]\n",
    "        station_reports = compute_delayed(station_observations, at_station)\n",
    "        delayed_reports.append(station_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_reports[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_compute = time.time()\n",
    "reports = dask.compute(*delayed_reports)\n",
    "print('Reports took {} to compute'.format(time.time() - begin_compute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = [report for station_reports in reports for report in station_reports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reports)\n",
    "df['hours'] = df['step'].dt.total_seconds() / 3600\n",
    "df['pass'] = df['date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error'] = df['gdps_temp'] - df['obs_temp']\n",
    "df['squared_error'] = df['error']**2\n",
    "df['abs_error'] = df['error'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['date'] == datetime.datetime(2020, 7, 20, 0)) & (df['hours'] < 48)].groupby('hours').mean()['abs_error'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='hours', y='abs_error', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='pass', y='abs_error', data=df, showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('station').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='lat', y='abs_error', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

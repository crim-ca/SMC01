{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275f1504-7399-4438-a6cb-a4746974f3de",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724e158-748e-4e84-9994-e9b105debf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4d766-8a06-4fee-91eb-3a6ebc1e42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask.distributed\n",
    "import dask_jobqueue\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import dask_xgboost\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12013346-91e2-4eee-a5ae-9291d9704d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81670260-ca54-4ad9-a3c7-02339c1d311b",
   "metadata": {},
   "source": [
    "## 0.0 Start Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41362f-2ed2-4c33-acb5-34fc1796c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.SLURMCluster(config_name='slurm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6231c1-6fa1-496d-b969-9a03c30ad8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba075d-a204-4655-8788-f528650035e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767c38e-5824-420f-9459-7647616231e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5dd75-0987-4425-9cc1-e5416e66d066",
   "metadata": {},
   "source": [
    "## 1.0 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1dbce5-a503-41ed-b9b0-1a0580d74287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328d4cb-0a5a-4b8e-867e-0008e119ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b781cc8-3b1b-46cf-a2bd-4c38e48ef440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9423a-82d3-41cd-8132-9f9603b6debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2f53f-0f68-4dfa-8f10-82f368d81002",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47624b2-696b-47fe-b030-f663fe5cf7eb",
   "metadata": {},
   "source": [
    "## 2.0 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498ee78-f949-497d-84f3-25774218c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error_2t'] = df['gdps_2t'] - df['obs_2t']\n",
    "df['step_hour'] = df['step'] / 3600\n",
    "df['step_td'] = dd.to_timedelta(df['step'], unit='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4c44f-6b1d-4ffa-a20e-ec638895e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e94045-5fc1-4b86-9334-a7a8f38e72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = []\n",
    "cont_columns = ['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', \n",
    "                'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', \n",
    "                'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick']\n",
    "target = 'error_2t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424bf07-3dbe-47ea-9691-8e55d95e49a8",
   "metadata": {},
   "source": [
    "* Split temporel naïf: on prendre X premières semaines pour train, X suivantes pour valid, X suivantes pour test\n",
    "* 70/10/20\n",
    "* shuffle=False pour garder l'ordre temporel\n",
    "* **Even with shuffle=False, it seems that the way dask parallelize the task still shuffle the instances.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe09e32e-525e-4c2e-9a81-bf6186106fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['date'] + cat_columns + cont_columns]\n",
    "y = df[target]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.30, shuffle=False)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=2/3, shuffle=False)\n",
    "\n",
    "train_counts = X_train.date.value_counts().compute()\n",
    "valid_counts = X_valid.date.value_counts().compute()\n",
    "test_counts = X_test.date.value_counts().compute()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train_counts.index, train_counts, s=1, c='g')\n",
    "ax.scatter(valid_counts.index, valid_counts, s=1, c='b')\n",
    "ax.scatter(test_counts.index, test_counts, s=1, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73ef02-08d4-4f62-9fd2-ebc5707f55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('date', axis=1)\n",
    "X_valid = X_valid.drop('date', axis=1)\n",
    "X_test = X_test.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ec6f6-a8d1-478c-af36-5327b34aee28",
   "metadata": {},
   "source": [
    "* Deal with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa577e-c601-44a7-a359-a064ffa0f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not X.isnull().values.any(), \"There are NaN values in the dataframe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1c36e-0b04-4c1b-8357-2b9f03626e8c",
   "metadata": {},
   "source": [
    "* We do not find all stations that we trained on in the valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59d564-ce75-42ae-9030-014e6e535142",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stations = df.loc[X_train.index].station.unique()\n",
    "valid_stations = df.loc[X_valid.index].station.unique()\n",
    "test_stations = df.loc[X_test.index].station.unique()\n",
    "print(f\"{len(set(train_stations) - set(valid_stations))} stations not in valid set\")\n",
    "print(f\"{len(set(train_stations) - set(test_stations))} stations not in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29678160-20a4-41e5-aafb-f5f00463feb5",
   "metadata": {},
   "source": [
    "## 3.0 Training\n",
    "\n",
    "### 3.0.0 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af463566-cfe9-4436-8419-cd811d1a75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d50ed-2fb4-4b4a-b1e0-38153b52fc47",
   "metadata": {},
   "source": [
    "* En moyenne, la prédiction est off de 2.98 degC par rapport à la valeur observée. La médiane est de 3.01 degC d'erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716eea1e-726c-468d-920a-30be0c6d304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = df.loc[X_train.index].error_2t\n",
    "abs(errors).describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e5e32-fe97-4f4e-a8ce-7fbcbbff916e",
   "metadata": {},
   "source": [
    "* No error (error_2t=0) can be a valid baseline, without abs() the mean is basically zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38d4a0-e369-457d-8511-4df0dee05a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines['baseline_zero'] = 0\n",
    "errors.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee40d6b-e38d-4e34-8e37-c7ca5acea53f",
   "metadata": {},
   "source": [
    "* La moyenne/médiane sur toutes les stations (train set) est un première baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e38c5-144e-4996-b4ab-247cecdb5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines['baseline_mean_pos'] = abs(errors).mean().compute()\n",
    "baselines['baseline_mean_neg'] = -abs(errors).mean().compute()\n",
    "baselines['baseline_median_pos'] = abs(errors).describe().compute().loc['50%']\n",
    "baselines['baseline_median_neg'] = -abs(errors).describe().compute().loc['50%']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0bdfe-daa3-48d5-8a37-8ccfbfff7af7",
   "metadata": {},
   "source": [
    "* La moyenne/médiane par station (train set) est un deuxième baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8536707-5fb1-4895-8cd2-6c018106a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines['baseline_station_mean_pos'] = abs(df.loc[X_train.index].groupby('station').error_2t.mean().compute())\n",
    "baselines['baseline_station_mean_neg'] = -abs(df.loc[X_train.index].groupby('station').error_2t.mean().compute())\n",
    "# Not working with parallelization\n",
    "# baselines['baseline_station_median_pos'] = abs(df.iloc[X_train.index].groupby('station').error_2t.median())\n",
    "# baselines['baseline_station_median_neg'] = -abs(df.iloc[X_train.index].groupby('station').error_2t.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9314f2-9dc7-4aad-90c4-cef3227865ab",
   "metadata": {},
   "source": [
    "* Prendre l'erreur de l'année précédente pour un tuple (station, date, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf4ddc-3208-4b6c-96a0-0070abd96081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not enough data analyzed for now because of memories issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21175428-15e1-4306-bb4d-368e515fddcb",
   "metadata": {},
   "source": [
    "* Compute metrics for each baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828dac8-1e27-4364-a8f4-874d010d2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in baselines.items():\n",
    "    if 'station' in k:\n",
    "        predictions_ = dd.merge(df.loc[X_test.index], v, on='station', suffixes=('', '_pred'), how='left').error_2t_pred.compute()\n",
    "        print(f'{predictions_.isna().sum()} NaN values: fill with 0.')\n",
    "        predictions_ = predictions_.fillna(0)\n",
    "    else:\n",
    "        predictions_ = np.full(len(X_test), v)\n",
    "    print(k)\n",
    "    print(f'\\tMAE: {mean_absolute_error(y_test, predictions_)}')\n",
    "    print(f'\\tRMSE: {mean_squared_error(y_test, predictions_, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7ceb3-fa97-4864-8d8d-7e170109e7fd",
   "metadata": {},
   "source": [
    "### 3.0.1 Train XGboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998451db-0e43-40ff-b636-555e91ddb3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "dvalid = xgb.dask.DaskDMatrix(client, X_valid, y_valid)\n",
    "dtest = xgb.dask.DaskDMatrix(client, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1531a-823e-4652-bfab-32f0a790d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 100\n",
    "params = {'objective': 'reg:squarederror', \n",
    "          'tree_method': 'hist',\n",
    "          'eval_metric': ['rmse', 'mae'],\n",
    "          'eta': 0.3\n",
    "         }\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cca4b-2a62-4459-ba9f-74922673d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.dask.train(client, params, dtrain, num_boost_round, evals=watchlist, verbose_eval=10, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c07388-cfeb-46da-8f42-2b9c0d225f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(model['history']['train']['rmse'], label='Train')\n",
    "plt.plot(model['history']['valid']['rmse'], label='Valid')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6812c-d697-4d91-9593-61b5c19294d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model['booster']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713bd62-00c6-4bec-b662-284878ff34b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb.dask.predict(client, model, dtest)\n",
    "print(f'MAE: {mean_absolute_error(y_test, predictions)}')\n",
    "print(f'RMSE: {mean_squared_error(y_test, predictions, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c6e0b-0d82-4e77-a597-8f7822cdf63f",
   "metadata": {},
   "source": [
    "### 3.0.2 Train XGboost model, split temporal by dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29b286-c4e5-48da-a1a4-4a830bfb75a9",
   "metadata": {},
   "source": [
    "* Let's keep 2019-01-01 to 2020-07-01 as train set (18 months)\n",
    "* 2020-07-01 to 2020-10-01 as valid set (3 months)\n",
    "* 2020-10-01 to 2020-12-31 as test set (3 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f2d38-1127-45ec-9f67-b0bf0daf1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'min: {df.date.min().compute()}')\n",
    "print(f'max: {df.date.max().compute()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22efcb-345e-4276-8ad2-db1bd9fd27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['date'] + cat_columns + cont_columns]\n",
    "y = df[target]\n",
    "\n",
    "train_idx = df[df.date <= '2020-07-01'].index\n",
    "valid_idx = df[(df.date > '2020-07-01') & (df.date <= '2020-10-01')].index\n",
    "test_idx = df[df.date > '2020-10-01'].index\n",
    "\n",
    "X_train = X.loc[train_idx]\n",
    "y_train = y.loc[train_idx]\n",
    "X_valid = X.loc[valid_idx]\n",
    "y_valid = y.loc[valid_idx]\n",
    "X_test = X.loc[test_idx]\n",
    "y_test = y.loc[test_idx]\n",
    "\n",
    "print(f'train: {len(X_train)/len(X)}, valid: {len(X_valid)/len(X)}, test: {len(X_test)/len(X)}')\n",
    "\n",
    "train_counts = X_train.date.value_counts().compute()\n",
    "valid_counts = X_valid.date.value_counts().compute()\n",
    "test_counts = X_test.date.value_counts().compute()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train_counts.index, train_counts, s=1, c='g')\n",
    "ax.scatter(valid_counts.index, valid_counts, s=1, c='b')\n",
    "ax.scatter(test_counts.index, test_counts, s=1, c='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a416c2-cc8e-426f-bf58-002f295c2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('date', axis=1)\n",
    "X_valid = X_valid.drop('date', axis=1)\n",
    "X_test = X_test.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa6633-d0f2-41f8-b5e0-c95b636a7458",
   "metadata": {},
   "source": [
    "* We do not find all stations that we trained on in the valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a05fd-aab6-4d24-8f4f-3e525f1acc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stations = df.loc[X_train.index].station.unique()\n",
    "valid_stations = df.loc[X_valid.index].station.unique()\n",
    "test_stations = df.loc[X_test.index].station.unique()\n",
    "print(f\"{len(set(train_stations) - set(valid_stations))} stations not in valid set\")\n",
    "print(f\"{len(set(train_stations) - set(test_stations))} stations not in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34beeaf2-8f1e-439b-8011-4fab56c46459",
   "metadata": {},
   "source": [
    "* DMatrix is an internal data structure that is used by XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d29c7-16d6-4a6e-aba8-40d94098cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "dvalid = xgb.dask.DaskDMatrix(client, X_valid, y_valid)\n",
    "dtest = xgb.dask.DaskDMatrix(client, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51565cd-298a-405b-9b31-fd8822ea5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 100\n",
    "params = {'objective': 'reg:squarederror', \n",
    "          'tree_method': 'hist',\n",
    "          'eval_metric': ['rmse', 'mae'],\n",
    "          'eta': 0.3\n",
    "         }\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bced7cc-5800-4f04-bab9-88d760831e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.dask.train(client, params, dtrain, num_boost_round, evals=watchlist, verbose_eval=10, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd97cc-31f5-4f5c-bf54-fb3634623099",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(model['history']['train']['rmse'], label='Train')\n",
    "plt.plot(model['history']['valid']['rmse'], label='Valid')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e436f-7462-4033-aca9-9f1b2eef83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model['booster']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30a917-3803-4563-ad25-8e8835f81c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb.dask.predict(client, model, dtest)\n",
    "print(f'MAE: {mean_absolute_error(y_test, predictions)}')\n",
    "print(f'RMSE: {mean_squared_error(y_test, predictions, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092dac5-a819-4ea1-a761-a3f81065db3d",
   "metadata": {},
   "source": [
    "### 3.0.3 Train XGboost model, backtesting\n",
    "\n",
    "* Let's train with X months, validate with 3 months and test with 3 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4679a-0f99-42c0-93d6-19b9c7c8cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'min: {df.date.min().compute()}')\n",
    "print(f'max: {df.date.max().compute()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c666969-3cf0-4a1a-8664-35de612f4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(df, train_start=3, period=3, valid_split=3, test_split=3,\n",
    "             cat_columns=[], cont_columns=['gdps_prate', 'gdps_prmsl', 'gdps_2t', 'gdps_2d', 'gdps_2r', 'gdps_10u', 'gdps_10v', 'gdps_10si', 'gdps_10wdir', 'gdps_al', 'gdps_t_850', 'gdps_t_500', 'gdps_gh_1000', 'gdps_gh_850', 'gdps_gh_500', 'gdps_u_500', 'gdps_v_500', 'gdps_q_850', 'gdps_q_500', 'gdps_thick'], target='error_2t',\n",
    "             num_boost_round=100, params={'objective': 'reg:squarederror', 'tree_method': 'hist', 'eval_metric': ['rmse', 'mae'], 'eta': 0.3}):\n",
    "    \n",
    "    # Isolate selected features\n",
    "    X = df[['date'] + cat_columns + cont_columns]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Get min and max date\n",
    "    start_date = df.date.min().compute()\n",
    "    end_date = df.date.max().compute()\n",
    "    total_months = round(((end_date - start_date)/np.timedelta64(1, 'M')))\n",
    "    \n",
    "    # Dict of splits\n",
    "    split_idx = np.arange(0, total_months-test_split-valid_split, period)\n",
    "    split_definitions = dict.fromkeys(range(len(split_idx)))\n",
    "    \n",
    "    for idx in split_idx:\n",
    "        backtest_id = int(idx/period)\n",
    "        print(f'Backtest {backtest_id}...')\n",
    "        \n",
    "        # Get split indices\n",
    "        train_end_date = start_date + datetime.timedelta(days=train_start*30) + datetime.timedelta(days=int(idx*30))\n",
    "        train_idx = df[(df.date >= start_date) & (df.date <= train_end_date)].index\n",
    "        valid_end_date = train_end_date + datetime.timedelta(days=valid_split*30)\n",
    "        valid_idx = df[(df.date > train_end_date) & (df.date <= valid_end_date)].index\n",
    "        test_end_date = valid_end_date + datetime.timedelta(days=test_split*30)\n",
    "        test_idx = df[(df.date > valid_end_date) & (df.date <= test_end_date)].index\n",
    "        \n",
    "        # Save backtest split dates\n",
    "        bt_dict = dict()\n",
    "        bt_dict['train_start'] = start_date\n",
    "        bt_dict['train_end_date'] = train_end_date\n",
    "        bt_dict['valid_start'] = train_end_date\n",
    "        bt_dict['valid_end_date'] = valid_end_date\n",
    "        bt_dict['test_start'] = valid_end_date\n",
    "        bt_dict['test_end_date'] = test_end_date\n",
    "        split_definitions[backtest_id] = bt_dict\n",
    "        \n",
    "        # Split \n",
    "        X_train = X.loc[train_idx]\n",
    "        y_train = y.loc[train_idx]\n",
    "        X_valid = X.loc[valid_idx]\n",
    "        y_valid = y.loc[valid_idx]\n",
    "        X_test = X.loc[test_idx]\n",
    "        y_test = y.loc[test_idx]\n",
    "        print(f'train: {len(X_train)/len(X)}, valid: {len(X_valid)/len(X)}, test: {len(X_test)/len(X)}')\n",
    "        \n",
    "        # Split validation\n",
    "        train_counts = X_train.date.value_counts().compute()\n",
    "        valid_counts = X_valid.date.value_counts().compute()\n",
    "        test_counts = X_test.date.value_counts().compute()\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(train_counts.index, train_counts, s=1, c='g')\n",
    "        ax.scatter(valid_counts.index, valid_counts, s=1, c='b')\n",
    "        ax.scatter(test_counts.index, test_counts, s=1, c='r')\n",
    "        plt.show();\n",
    "        \n",
    "        X_train = X_train.drop('date', axis=1)\n",
    "        X_valid = X_valid.drop('date', axis=1)\n",
    "        X_test = X_test.drop('date', axis=1)\n",
    "        \n",
    "        # Convert to DMatrix for xgboost\n",
    "        dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "        dvalid = xgb.dask.DaskDMatrix(client, X_valid, y_valid)\n",
    "        dtest = xgb.dask.DaskDMatrix(client, X_test)\n",
    "        \n",
    "        # Train XGBoost\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "        model = xgb.dask.train(client, params, dtrain, num_boost_round, evals=watchlist, verbose_eval=10, early_stopping_rounds=10)\n",
    "        \n",
    "        # Train validation\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(model['history']['train']['rmse'], label='Train')\n",
    "        plt.plot(model['history']['valid']['rmse'], label='Valid')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('RMSE Loss')\n",
    "        plt.show();\n",
    "        \n",
    "        # Feature importance\n",
    "        xgb.plot_importance(model['booster'])\n",
    "        plt.show();\n",
    "        \n",
    "        # Performance\n",
    "        predictions = xgb.dask.predict(client, model, dtest)\n",
    "        print(f'MAE: {mean_absolute_error(y_test, predictions)}')\n",
    "        print(f'RMSE: {mean_squared_error(y_test, predictions, squared=False)}')\n",
    "    \n",
    "    return split_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de10a6-a5bb-4aaa-bd62-d2127baeff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = backtest(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6320b-1985-4a31-a05c-73624c717430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smc01-xgb)",
   "language": "python",
   "name": "smc01-xgb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

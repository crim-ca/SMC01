{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debias\n",
    "\n",
    "Perform a basic *monthly debiasing* on GDPS and compare it to the raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask.distributed\n",
    "import dask_jobqueue\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(os.getenv(\"DATA_DIR\"))\n",
    "INPUT_DATASET = DATA_DIR / pathlib.Path('2021-12-20-gdps-metar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = pathlib.Path(INPUT_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = next(iter(input_path.glob('*.parquet')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_parquet(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = set(sample.columns)\n",
    "columns -= set(['gdps_hpbl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDPS HPBL is unavailable from 2019010100 to 2019031100. I think it's because the field was thinned out before we asked for it to be dearchived.\n",
    "The partially available column causes problems in this notebook, so I remove the column.\n",
    "Eventually we should make sure a gdps_hpbl with nans is available in all the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.SLURMCluster(\n",
    "    env_extra=['source ~/.bash_profile','conda activate smc01'],\n",
    "    name='smc01-dask',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = sorted(list(input_path.glob('*.parquet')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(dataset_files, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['step_hour'] = df['step'] / 3600\n",
    "df['error_2t'] = df['obs_2t'] - df['gdps_2t']\n",
    "df['squared_error_2t'] = (df['gdps_2t'] - df['obs_2t']) ** 2\n",
    "df['rmse_2t'] = da.sqrt(df['squared_error_2t'])\n",
    "df['mabs_2t'] = np.abs(df['error_2t'])\n",
    "df['forecast_month'] = df['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['step_td'] = dd.to_timedelta(df['step'], unit='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['step_td']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc['2019-01-01':'2021-01-01']\n",
    "val = df.loc['2021-01-01':'2022-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_bias = train.groupby(['station', 'forecast_month', 'step_hour']).agg({'error_2t': 'mean'}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_bias = monthly_bias.rename(columns={'error_2t': 'bias_2t'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_bias = monthly_bias.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(monthly_bias, on=['station', 'forecast_month', 'step_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['gdps_corrected_2t'] = train['gdps_2t'] + train['bias_2t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['gdps_corrected_2t_error'] = train['obs_2t'] - train['gdps_corrected_2t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['gdps_corrected_2t_se'] = train['gdps_corrected_2t_error'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = train.groupby('step_hour').agg({'gdps_corrected_2t_se': 'mean', 'squared_error_2t': 'mean'}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = errors.reset_index().melt(id_vars=['step_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(data_frame=errors, x='step_hour', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val.merge(monthly_bias, on=['station', 'forecast_month', 'step_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['gdps_corrected_2t'] = val['gdps_2t'] + val['bias_2t']\n",
    "val['gdps_corrected_2t_error'] = val['obs_2t'] - val['gdps_corrected_2t']\n",
    "val['gdps_corrected_2t_se'] = val['gdps_corrected_2t_error'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = val.groupby('step_hour').agg({'gdps_corrected_2t_se': 'mean', 'squared_error_2t': 'mean'}).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_errors = errors.reset_index().melt(id_vars=['step_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(data_frame=melt_errors, x='step_hour', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors['skill_score'] = 1.0 - (errors['gdps_corrected_2t_se'] / errors['squared_error_2t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(data_frame=errors, x=errors.index, y='skill_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of a monthly debiasing model decreases pretty linearly with time.\n",
    "I suspect all of our models will have the same phenomenon.\n",
    "Next model to try is probably an EMOS model, but this will be a little more involved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMC01",
   "language": "python",
   "name": "smc01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
